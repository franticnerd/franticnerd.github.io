'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/environment/','title':"Environment",'content':""});index.add({'id':1,'href':'/docs/environment/env-aws-docker/','title':"Env Aws Docker",'content':"Docker in AWS EC2 We developed a Docker image which pre-installed all modules in this bootcamp. You can directly use it in your own environment if you have docker. This page describes how to launch an EC2 instance on AWS and run docker container within it.\nLaunch an AWS EC2 instance  Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ From the Amazon EC2 console dashboard, click AMIs on left sidebar. Switch Region to N. Virginia if you are in other regions. Choose Public Images in dropdown below launch and search for ami-59d4f433. Select the image and click the blue Launch button. On the Choose an Instance Type page, select the hardware configuration and size of the instance to launch. Choose the type m4.xlarge, with 4 vCPUs and 16GB memory, then click “Next: Configuration Instance Details”. On the Configure Instance Details page, just keep the default settings. On the Add Storage page, you can specify storage size for your disk. Use the default 30GB. On the Tag Instance page, specify tags for your instance by providing key value combinations if you need. On the Configure Security Group page, define firewall rules for your instance. We suggest you\u0026rsquo;d better keep the default setting unless you are sure what you are doing. On the Review Instance Launch page, check the details of your instance and click Launch. In the Select an existing key pair or create a new key pair dialog box. If you don’t have an existing key pair, choose create a new key pair. Enter a name for the key pair (e.g. bdhKeyPair) and click “Download Key Pair”. This key pair will be used to connect to your instance. Then on the same dialog box, choose Choose an existing key pair, and select the one you just created. Finally, click “Launch Instances”. You can view your instances by clicking Instances on the left navigation bar.  Connect to the instance After your instance is fully launched, you can connect to it using SSH client. Right click on the instance and click connect then AWS will show you instructions about connecting on various platform. ssh command will be used for *nix platform and Putty will be used for windows.\nStart a docker container A pre-configured container with all necessary module installed is available for you to directly use. Navigate to ~/lab/docker and vagrant up will launch a container like below\nLast login: Mon Jan 25 03:29:38 2016 from lawn-128-61-36-142.lawn.gatech.edu __| __|_ ) _| ( / Amazon Linux AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-ami/2015.09-release-notes/ 20 package(s) needed for security, out of 38 available Run \u0026#34;sudo yum update\u0026#34; to apply all updates. [ec2-user@ip-172-31-23-158 ~]$ cd lab/docker/ [ec2-user@ip-172-31-23-158 docker]$ vagrant up Bringing machine \u0026#39;bootcamp1\u0026#39; up with \u0026#39;docker\u0026#39; provider... ==\u0026gt; bootcamp1: Creating the container... bootcamp1: Name: docker_bootcamp1_1453695135 bootcamp1: Image: sunlab/bigdata:0.04 bootcamp1: Volume: /home/ec2-user/lab/bigdata-bootcamp:/home/ec2-user/bigdata-bootcamp bootcamp1: Volume: /home/ec2-user/lab/scripts:/home/ec2-user/bigdata-scripts bootcamp1: Volume: /home/ec2-user/lab/docker:/vagrant bootcamp1: bootcamp1: Container created: cc2f518631e86a11 ==\u0026gt; bootcamp1: Starting container... ==\u0026gt; bootcamp1: Provisioners will not be run since container doesn\u0026#39;t support SSH. [ec2-user@ip-172-31-23-158 docker]$ vagrant ssh Last login: Thu Jan 21 20:59:15 2016 from ip-172-17-0-1.ec2.internal [ec2-user@bootcamp1 ~]$ Then start all hadoop related service by\n[ec2-user@bootcamp1 ~]$ bigdata-scripts/start-all.sh JMX enabled by default Using config: /etc/zookeeper/conf/zoo.cfg Starting zookeeper ... STARTED starting proxyserver, logging to /var/log/hadoop-yarn/yarn-yarn-proxyserver-bootcamp.local.out Started Hadoop proxyserver: [ OK ] starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-bootcamp.local.out Started Hadoop namenode: [ OK ] starting datanode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-datanode-bootcamp.local.out Started Hadoop datanode (hadoop-hdfs-datanode): [ OK ] starting resourcemanager, logging to /var/log/hadoop-yarn/yarn-yarn-resourcemanager-bootcamp.local.out Started Hadoop resourcemanager: [ OK ] starting historyserver, logging to /var/log/hadoop-mapreduce/mapred-mapred-historyserver-bootcamp.local.out Started Hadoop historyserver: [ OK ] starting nodemanager, logging to /var/log/hadoop-yarn/yarn-yarn-nodemanager-bootcamp.local.out Started Hadoop nodemanager: [ OK ] Starting Spark worker (spark-worker): [ OK ] Starting Spark master (spark-master): [ OK ] Starting Hadoop HBase regionserver daemon: starting regionserver, logging to /var/log/hbase/hbase-hbase-regionserver-bootcamp.local.out hbase-regionserver. starting master, logging to /var/log/hbase/hbase-hbase-master-bootcamp.local.out Started HBase master daemon (hbase-master): [ OK ] starting thrift, logging to /var/log/hbase/hbase-hbase-thrift-bootcamp.local.out Started HBase thrift daemon (hbase-thrift): [ OK ] [ec2-user@bootcamp1 ~]$ Termination You can terminate a docker by vagrant destroy --force in ~/lab/docker/.\nLimitations After a docker container exit, you may loose data stored within it. You can map folder from AWS EC2 instance with Docker container for persistent data saving.\n"});index.add({'id':2,'href':'/docs/environment/env-azure-docker/','title':"Env Azure Docker",'content':"Docker in Azure We could use Azure as a virtual machine provider. If you have no enough resource to host our envornment in local, you can also launch an Azure instance, start a docker service inside, and host our docker image.\nWe can create a Docker on Ubuntu Server in Azure, and then pull image from hub.docker.com.\nLaunch an Azure instance Option 1: Launch a Pre-installed Docker Host \u0026ldquo;Docker on Ubuntu Server\u0026rdquo; is a container based on Ubuntu Server 16.04 LTS published by Canonical. You can launch a new ”Docker on Ubuntu Server” instance, and you will able to start your docker directly.\n Open the Portal in Azure at https://portal.azure.com Click Virtual Machines on the left sidebar Click “+ ADD” to create a new instance Type “docker” in search box, and select \u0026ldquo;Docker on Ubuntu Server\u0026rdquo; Click “Create” on the introduction page Fill your host name, user name, authentication Click Pricing Tier, and choose D2S_V3 Click “create” to create the instance  ::: tip\nThe D2S_V3 is just an example. You may choose anyone fulfill the requirement ( RAM may greater than 8G, CPU should more than 1).\n:::\nOption 2: Launch a clear linux \u0026ldquo;Docker on Ubuntu Server\u0026rdquo; is using \u0026ldquo;classic deployment\u0026quot;. It seems like there are too few choise in \u0026ldquo;classic deployment\u0026rdquo; now. If you wish to have different option. Or, if you are more familiar with CentOS, SUSE or some other distributions, you can simply choose them.\nIn this case, you could unlocked more options in Azure Instance Type. Whatever your choise is, it is just a docker container, which does not matter the detail in your environment.\n Open the Portal in Azure at https://portal.azure.com Click Virtual Machines on the left sidebar Click “+ ADD” to create a new instance Type “CentOS”, or \u0026ldquo;Ubuntu\u0026rdquo; in search box, and select any image you like. For example, I would like to choose \u0026ldquo;Ubuntu Server 16.04 LTS\u0026rdquo; here You should able to find a drop down box between \u0026ldquo;Select a deployment mode\u0026rdquo; and \u0026ldquo;Create\u0026rdquo; which should has 2 options \u0026ldquo;Resource Manager\u0026rdquo; and \u0026ldquo;Classic\u0026rdquo;. Please make sure it is on \u0026ldquo;Resource Manager\u0026rdquo; Click “Create” on the introduction page Fill your host name, user name, authentication Click \u0026ldquo;Ok\u0026rdquo;, and you should see a few options for your virtual machine. For this course, I would suggest you to choose a instance has 8GB or 16GB RAM and 2-4 vCPUs Fill in the rest of the information by yourself, and finally click “create” to create the instance  Connect to the instance  Open the Portal in Azure at https://portal.azure.com Click All resources on left sidebar. Select your instances from “Virtual machines (classic)”, “Cloud service” or \u0026ldquo;Virtual machines\u0026rdquo; depends on your choise find Public IP addresses in “Overview” Login via command “ssh your-username@public-ip” in *nix or using putty for windows (If you are using option 2), Install Docker in Linux  Start a docker container Most of the related application are already installed, you can also install other apps with command “apt-get”.\nFor example:\nsudo apt-get install git tmux\rAnd then, start a new docker instance\nsudo docker run -it --privileged=true \\\r --cap-add=SYS_ADMIN \\\r -m 6144m -h bootcamp.local \\\r --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\\r -v /:/mnt/host \\\r sunlab/bigbox:latest \\\r /bin/bash\rPlease refer to this section for some detail information of docker.\nIf you are interested, you may also use docker-compose to manage the service easier.\nLogin remotely and the corresponding security issue To access the service we launched in Azure, we are required to get the corresponding port opened. In default, the ports are closed. A good way to open it is change the rule of filewall. Please refer to this official document to open the port.\nHowever, since this image was designed to start a easy learning environment, if you keep the ports open to the public, your environment could be hacked easily. You can also making full use of the SSH port forwarding.\nFor example, you can create a new terminal, and type a command as follow:\nssh -L 2222:localhost:2222 \\\r-L 8888:localhost:8888 \\\ryour-azure-user@your-azure-host\rThis command will connect to your azure VM. In the meantime, It will also forward the network steam from azure:{2222, 8888}. And then, you can visit localhost:8888 to visit jupiter if you have your jupiter in azure started.\n::: tip\n The spending in Azure is calculated by time usage. Launch a better instance and fully destroy it instance once you finish your job will reduce your time in any dimension You may use tmux to make your life better  :::\n"});index.add({'id':3,'href':'/docs/environment/env-docker-compose/','title':"Env Docker Compose",'content':"Docker Compose ::: tip This is an optional section. Docker Compose is just a utility, which does NOT affect the functionality of the docker image and our course.\nYou can simply ignore it if you believe the docker commands are enough. :::\nDocker Compose is a tool for defining and running multi-container Docker applications. We can write a simple docker-compose.yml file as configure file, and launch a docker container as a service easily.\nThis means you can fix all your parameters in your configure file, and start/stop them using more biref commands.\nOfficial Guides  Compose Overview Install Compose Getting Started Compose command-line reference Compose file version 3 reference  Compose file structure and examples   Environment file  Install Docker Compose If you are a Windows/macOS user, the docker-compose should installed with your docker application.\nIf this command is missing in your machine\n# docker-compose -bash: docker-compose: command not found It is also pretty easy.\nPlease visit the Docker Compose Release Page, following the guide and download the latest binary release.\nCreate A docker-compose.yml You can go to an empty folder in somewhere, and create a text file named as docker-compose.yml. The content may as follow:\nversion: \u0026#39;3\u0026#39; services: bootcamp: image: sunlab/bigbox:latest hostname: bootcamp domainname: local restart: \u0026#34;no\u0026#34; volumes: # Volumes section defined the mappings between host machine and # virtual machine. # \u0026#34;:\u0026#34; split each element into 2 parts # the left part is the folder in host machine # the right part is the folder in virtual machine # docker-compose support relative path # Please refer to # https://docs.docker.com/compose/compose-file/#volumes # for more detail if you are interested - ./data/logs:/var/log - ./data/host:/mnt/host environment: - CONTAINER=docker # /scripts/entrypoint.sh will start all the services # and then finally listen to port 22. command: [ \u0026#34;/scripts/entrypoint.sh\u0026#34; ] ports: # Ports section defined a few rules and forward the network # stream between host machine and vm. # As the rules in volumes section # The left part is for your host machine. # This means you can visit localhost:2333 # and then get the response from the app # listening port 22 in docker container  - \u0026#34;2333:22\u0026#34; - \u0026#34;7077:7077\u0026#34; # spark - \u0026#34;4040:4040\u0026#34; - \u0026#34;4041:4041\u0026#34; - \u0026#34;8888:8888\u0026#34; # - \u0026#34;8983:8983\u0026#34; # for solr Here is a example.\nBasic Operation Similar to other parts. If you are a linux user, you should always add an \u0026lsquo;sudo\u0026rsquo; before the command docker-compose.\nUp and Down, Start and Stop. How to control the virtual environment If you wish to create and start the container(s).\ndocker-compose up This command will search the configure file docker-compose.yml or docker-compose.yaml in current folder, and start the corresponding services.\nIt will pull the image described in the yml file if there is no local cache. Otherwise it will create a container through local image.\nIn this case, if you close the terminal, the service may be stopped but not destroyed.\nYou an pass a parameter -d. This parameter indices the docker-compose will run the containers in the background after started.\nPlease type the following command for more introduction.\ndocker-compose help up If you wish to clearly terminate everything. You may type\ndocker-compose down This command will stops containers and removes containers, networks, volumes, and images created by up.\nEverything inside the container are vanished.\nIf you have a container created. You may use docker-compose start/stop/restart to start/stop/restart the service. The container will stay over there\nRun, Exec and SSH. How to access the environment docker-compose run bootcamp bash run will launch a brand new container based on the configuration. bootcamp is name of the service in the config. This may useful in debugging or somewhere else, but may not fit for this course.\ndocker-compose exec bootcamp bash exec will reuse the container and execute a specific command. The command is bash in this case. You should make sure the service is already started before typing this command.\nYou can also access the environment through SSH.\nWe have also initialized the SSH RSA Keys here.\ncurl https://raw.githubusercontent.com/yuikns/bigbox/master/config/ssh/id_rsa -o bigbox-private-key chmod 0600 bigbox-private-key # prevent error: Permissions 0644 for \u0026#39;./bigbox-private-key\u0026#39; are too open. ssh -p 2333 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i bigbox-private-key root@127.0.0.1 If you are running your docker compose in remote. This identity file may unsafe. You can generate a pair by yourself.\n[root@bootcamp ~]# rm -rf ~/.ssh/ [root@bootcamp ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory \u0026#39;/root/.ssh\u0026#39;. Enter passphrase (empty for no passphrase): # type enter here Enter same passphrase again: # type enter here again Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:2NYgKW6bCmLvFPAug868hXBAIW6PhWlJeRK6LeYCHJ0 root@bootcamp.local The key\u0026#39;s randomart image is: +---[RSA 2048]----+ |.=+ | |*o=.. . | |oXoE. o . | |++B. . + o | |=+o+o . S . | |*oo..o . | |=* +o | |B.*. | | =+o | +----[SHA256]-----+ [root@bootcamp ~]# cd .ssh/ [root@bootcamp .ssh]# cp id_rsa.pub authorized_keys [root@bootcamp .ssh]# chmod 0600 authorized_keys [root@bootcamp .ssh]# ls -alh total 20K drwx------ 2 root root 4.0K Jun 20 08:42 . dr-xr-x--- 1 root root 4.0K Jun 20 08:41 .. -rw------- 1 root root 401 Jun 20 08:42 authorized_keys -rw------- 1 root root 1.7K Jun 20 08:41 id_rsa -rw-r--r-- 1 root root 401 Jun 20 08:41 id_rsa.pub You may copy the id_rsa file to local as your key file.\nLogs Logs for Hadoop Ecosystem You may noticed, the sample docker-compose.yml mapped the folder from /var/log in vm to ./data/logs in physical machine. You may simply check the files in your folder ./data/logs.\nLogs from docker container You may trace it using command:\ndocker-compose logs -f --tail=100 "});index.add({'id':4,'href':'/docs/environment/env-local-docker-linux/','title':"Env Local Docker Linux",'content':"Install Docker In Linux Install Docker on RHEL/CentOS/Fedora  Get Docker CE for CentOS Get Docker CE for Fedora  In brief, after updated your system, you can simply type the follow commands:\nsudo yum install docker-ce -y # install docker package sudo service docker start # start docker service chkconfig docker on # start up automatically FAQ  If your SELinux and BTRFS are on working, you may meet an error message as follow:  # systemctl status docker.service -l ... SELinux is not supported with the BTRFS graph driver! ... Modify /etc/sysconfig/docker as follow:\n# Modify these options if you want to change the way the docker daemon runs #OPTIONS=\u0026#39;--selinux-enabled\u0026#39; OPTIONS=\u0026#39;\u0026#39; ... Restart your docker service\nStorage Issue:  Error message found in /var/log/upstart/docker.log\n[graphdriver] using prior storage driver \\\u0026quot;btrfs\\\u0026quot;... Just delete directory /var/lib/docker and restart docker service\nInstall Docker on Ubuntu/Debian  Get Docker CE for Ubuntu Get Docker CE for Debian  Generally, you are supposed to add the repository, and then\nsudo apt-get install docker-ce Both Debian Series and RHEL Series can be controlled by\nsudo service docker start # stop, restart, ... Once you started your service, you would find a socket file /var/run/docker.sock, and then you would able to execute your docker commands.\n"});index.add({'id':5,'href':'/docs/environment/env-local-docker-macos/','title':"Env Local Docker Macos",'content':"Install Docker In macOS If you are using macOS, you are supposed to prepare a virtual machine/service to host a real Linux instance, and then control it in the remote.\nCurrently, there are at least 2 solutions to achieve it.\nOption One: Docker.app If you are using macOS, you could follow this guide. Download an image, and drag to your \u0026lsquo;Applications\u0026rsquo; folder, click and run.\n Stable channel Edge channel  Currently, both of them are working pretty fine.\n Kubernetes is only available in Docker for Mac 17.12 CE Edge and higher, on the Edge channel. Edge can not disable \u0026ldquo;Sending usage statistics\u0026rdquo; For more  Either of them could satisfy the requirements in this course.\nIf you wish to host our image using Docker.app, you are supposed to click the icon for docker in the toolbar, and set the maximum memory to 4G-8G.\nDocker.app requires sudo access, and the data will stored in $HOME/Library/Containers/com.docker.docker\nOption Two: Homebrew + VirtualBox + Docker However, here is an alternative solution.\nFirst of all, you should make sure you have already installed HomeBrew.\nSecondly, you are supposed to make sure your brew is up-to-date.\nbrew update # update brew repository brew upgrade # update all packages for brew brew doctor # check the status of your brew, it will provide some guide to make your brew be normal Finally, you can install VirtualBox and Docker by using the following commands:\nbrew install Caskroom/cask/virtualbox brew install docker-machine brew install docker To keep the Docker service active, we can use brew\u0026rsquo;s service manager\n$ brew services start docker-machine ==\u0026gt; Successfully started `docker-machine` (label: homebrew.mxcl.docker-machine) Check the status:\n$ brew services list Name Status User Plist docker-machine started name /Users/name/Library/LaunchAgents/homebrew.mxcl.docker-machine.plist Create a default instance using the following command:\ndocker-machine create --driver virtualbox --virtualbox-memory 8192 default Please refer to this link for some detail instruction.\nEvery time you created a new terminal window, and before you execute any command of \u0026lsquo;docker *', you are supposed to run the following command:\neval $(docker-machine env default) This command will append some environment variables to your current sessions.\nFAQ Q: Can not connect to docker Error Message:\n$ docker ps -a Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? Please make sure you have already started your session.\nQ: Start docker-machine Failed, Can Not Get IP Address The default manager by vbox is conflict with vpn AnyConnect, if you are using it, just disconnect it.\nQ: Invalid Active Developer Path xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun Error: Failure while executing: git config --local --replace-all homebrew.analyticsmessage true try xcode-select --install and then brew update, brew upgrade, and brew doctor again\nQ: Where are the data for the images and hard disks? They are in $HOME/.docker\n"});index.add({'id':6,'href':'/docs/environment/env-local-docker-windows/','title':"Env Local Docker Windows",'content':"Install Docker In Microsoft Windows ::: tip\nIf you are interested in software from Microsoft (excluding Microsoft Office products), this link may helps.\nYou may follow the guide, go to the Imagine login page to download the Windows image for free. :::\nWindows Docker Desktop (recommended) An official guide for Windows Docker Desktop could be found here.\nDownload the image from this link and follow the installer step by step.\nWindows Docker Desktop requires Windows 10 Pro or Enterprise version 1586/2016 RTM or above\nIf your windows is not fulfill the prerequisite, you may see the image as follow. Please use Docker Toolbox on Windows instead.\nOnce you successfully installed docker desktop, you may click the button \u0026ldquo;Docker Desktop\u0026rdquo;. It may take a few minutes to start the service. You may keep a watch on the whale on the right button.\nDouble click the whale button, we may able to find a window to modify some of the properties.\nPlease go to the advanced tab, and click the drivers you wish to share with the docker container. Note: if your homework is located on Disk-D, you may not select Disk-C, this could make your operation system more safe.\nGoing to the advanced tab, and you can edit the maximum memory used by docker.\nIf you can execute command docker ps -a and docker-compose normally, and no error message returned. Your configuration is fine.\nDocker Toolbox on Windows You may install Docker Toolbox on Windows instead.\nGoing to the instruction page, click \u0026lsquo;Get Docker Toolbox for Windows', you will download a installer. You are supposed to install Docker and VirtualBox during this wizard.\nClick \u0026lsquo;Docker Quickstart Terminal\u0026rsquo;, you should able to start a bash session. Close it, click virtual box. You may find there is one virtual machine is in running. Close this machine, update the maximum base memory.\nClick the \u0026lsquo;Docker Quickstart Terminal\u0026rsquo; and your docker is ready.\nFAQ Q: VirtualBox won\u0026rsquo;t boot a 64bits VM when Hyper-V is activated You may meet message as follow:\n Error with pre-create check: \u0026ldquo;This computer is running Hyper-V. VirtualBox won\u0026rsquo;t boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use \u0026ndash;virtualbox-no-vtx-check)\n You can’t run VirtualBox on a Hyper-V enabled system. Hyper-V is a tier-1 hypervisor, which doesn’t accept other hypervisors (from here)\n It seems like Docker for Windows has already resolved this issue Try to disable Hyper-V. Caution: According to some reports, this operation may damage his/her network and had to reinstall all network adapters to get Internet back, or even getting a blue screen error. Try to use hyperv as your backend driver. https://docs.docker.com/machine/drivers/hyper-v/  "});index.add({'id':7,'href':'/docs/environment/env-local-docker/','title':"Env Local Docker",'content':"Docker in Local OS ::: tip For the purpose of the environment normalization, we provide a simple docker image for you, which contains most of the software required by this course. We also provide a few scripts to install some optional packages. :::\nThe whole progress would seem as follow:\n Make sure you have enough resource:  It requires at least 8GB Physical RAM, 16GB or greater would be better It requires at least 15GB hard disk storage   Install a docker environment in local machine Start Docker Service, pull images and create a instance Just rock it! Destroy the containers and images if they are not required anymore  ::: warning Since this docker image integrated a lot of related services for the course, it requires at least 4GB RAM for this virtual machine. If your can not meet the minimum requirement, the system could randomly kill one or a few process due to resource limitation, which causes a lot of strange errors which is even unable to reproduce.\nDON\u0026rsquo;T TRY TO DO THAT.\nYou may try Azure instead. :::\n[[toc]]\n0. System Environment You should have enough system resource if you are planning to start a container in your local OS.\nYou are supposed to reserve at least 4 GB RAM for Docker, and some other memory for the host machine. While, you can still start all the Hadoop related services except Zeppelin, even if you only reserve 4GB for the virtual machine.\n1. Install Docker Docker is a software technology providing operating-system-level virtualization also known as containers, promoted by the company Docker, Inc.. Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent \u0026ldquo;containers\u0026rdquo; to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs). (from Wikipedia)\nBasically, you can treat docker as a lightweight virtual machine hosted on Linux with a pretty high performance.\nThe principle of setting up a docker environment is pretty straightforward.\n IF your operating system is Linux, you are supposed to install docker service directly IF your operating system is mac OS, Windows, FreeBSD, and so on, you are supposed to install a virtual machine, start a special configured Linux system which hosts a Docker service. You will control the dockers using remote tool  There is an official instruction from the link. You can check the official documentation to get the latest news and some detail explanations.\n Install Docker In Linux Install Docker In macOS Install Docker In Microsoft Windows  Once the docker installed, you should get a few commands start from docker and able to start your docker service, and launch your docker container.\n docker - a tool to control docker docker-machine - a tool that lets you install Docker Engine on virtual hosts, and manage the hosts in remote docker-compose - a tool for defining and running multi-container Docker applications  If we are using VirtualBox + Windows/macOS, the theory is pretty clear: we created a Linux instance in \u0026ldquo;virtual remote\u0026rdquo;, and control it using docker-machine. If we are supposed to operate the \u0026ldquo;remote docker service\u0026rdquo;, we are supposed to prepare a set of environment variables. We can list it using command:\ndocker-machine env default This is the reason that why do we have to execute the follow command to access the docker.\neval $(docker-machine env default) ::: tip\nIf you are using docker-machine, you can not reach the port from virtual machine using ip 127.0.0.1 (localhost). As replacement, you should extract the IP using this command:\n$ printenv | grep \u0026#34;DOCKER_HOST\u0026#34; DOCKER_HOST=tcp://192.168.99.100:2376 And then you should visit 192.168.99.100 instead of 127.0.0.1 to visit the network stream from virtual machine.\n:::\nIf these environment are unsetted, docker will try to connect to the default unix socket file /var/run/docker.sock.\nAs a Docker.app user, this file is:\n$ ls -alh /var/run/docker.sock lrwxr-xr-x 1 root daemon 55B Feb 10 19:09 /var/run/docker.sock -\u0026gt; /Users/yu/Library/Containers/com.docker.docker/Data/s60 $ ls -al /Users/yu/Library/Containers/com.docker.docker/Data/s60 srwxr-xr-x 1 yu staff 0 Feb 10 19:09 /Users/yu/Library/Containers/com.docker.docker/Data/s60 As a Linux user, the situation is slightly different:\n$ ls -al /var/run/docker.sock srw-rw---- 1 root root 0 Feb 11 11:35 /var/run/docker.sock ::: tip A Linux user must add a \u0026ldquo;sudo\u0026rdquo; before command docker since he has no access to docker.sock as an ordinary user. :::\n2. Pull and run Docker image (1) Start the container with: The basic start command should be:\ndocker run -it --privileged=true \\  --cap-add=SYS_ADMIN \\  -m 8192m -h bootcamp.local \\  --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\  -v /:/mnt/host \\  sunlab/bigbox:latest \\  /bin/bash In general, the synopsis of docker run is\ndocker run [options] image[:tag|@digest] [command] [args] Here is a case study to the options:\n -p host-port:vm-port\n This option is used to map the TCP port vm-port in the container to port host-port on the Docker host.\nCurrently, vm-ports are reserved to:\n 8888 - Jupyter Notebook 9530 - Zeppelin Notebook  Once you started the Zeppelin service, this service will keep listening port 9530 in docker. You should able to visit this service using http://127.0.0.1:9530 or http://DOCKER_HOST_IP:9530.\nThis remote IP depends on the Docker Service you are running, which has already described above.\n If you are using Linux or Docker.app in macOS, you just need to visit \u0026ldquo;localhost:9530\u0026rdquo;, or other port numbers if you changed host-port If you are using VirtualBox + macOS or Windows, you should get the Docker\u0026rsquo;s IP first   -v, \u0026ndash;volume=[host-src:]container-dest[:\u0026lt;options\u0026gt;]\n This option is used to bind mount a volume.\nCurrently, we are using -v /:/mnt/host. In this case, we can visit the root of your file system for your host machine. If you are using macOS, /mnt/host/Users/\u0026lt;yourname\u0026gt;/ would be the $HOME of your MacBook. If you are using Windows, you can reach your C: disk from /mnt/host/c in docker.\nVariable host-src accepts absolute path only.\n -it\n  -i : Keep STDIN open even if not attached -t : Allocate a pseudo-tty   -h bootcamp.local\n Once you enter this docker environment, you can ping this docker environment itself as bootcamp.local. This variable is used in some configuration files for Hadoop ecosystems.\n -m 8192m\n Memory limit (format: \u0026lt;number\u0026gt;[\u0026lt;unit\u0026gt;]). Number is a positive integer. Unit can be one of b, k, m, or g.\nThis docker image requires at least 4G RAM, 8G RAM is recommended. However, if your local Physical Machine has ONLY 8G RAM, you are recommended to reduce this number to 4G.\nLocal machine is not the same as the remote server. If you are launching a remote server with 8G RAM, you can set this number as 7G.\nIf you are interested in the detail explanation of the args, please visit this link\n(2) Start all necessary services In generally, when you are in front of the command line interface, you will meet 2 kinds of prompt.\n# whoami # this prompt is \u0026#39;#\u0026#39; #indices you are root aka the administrator of this environment now root $ whoami # this promot is \u0026#39;$\u0026#39; indices you are a ordinary user now yu Of course, it is pretty easy to change, you can simply update the environment variable PS1.\nAssumption: every script is executed by root.\n/scripts/start-services.sh This script will help you start a the services for Hadoop ecosystems. You may meet \u0026ldquo;Connection Refused\u0026rdquo; exception if you did something else before started these services.\nIf you wish to host Zeppelin, you should install it first by using the command:\n/scripts/install-zeppelin.sh and start the service by using command:\n/scripts/start-zeppelin.sh then, Zeppelin will listen the port 9530\nNote: Please keep all the service are running before installing/starting Zeppelin.\nIf you wish to host Jupyter, you can start it by using command:\n/scripts/start-jupyter.sh Jupyter will listen the port 8888\n(3) Stop all services You can stop services if you want:\n/scripts/stop-services.sh (4) Detach or Exit To detach instance for keeping it up,\nctrl + p, ctrl + q To exit,\nexit (5) Re-attach If you detached a instance and want to attach again, check the CONTAINER ID or NAMES of it.\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 011547e95ef5 sunlab/bigbox:latest \u0026#34;/tini -- /bin/bash\u0026#34; 6 hours ago Up 4 seconds 0.0.0.0:8888-\u0026gt;8888/tcp, 0.0.0.0:9530-\u0026gt;9530/tcp, 0.0.0.0:2222-\u0026gt;22/tcp bigbox If the \u0026ldquo;STATUS\u0026rdquo; column is similar to \u0026ldquo;Exited (0) 10 hours ago\u0026rdquo;, you are supposed to start the container again.\n$ docker start \u0026lt;CONTAINER ID or NAMES\u0026gt; Then attach it by:\n$ docker attach \u0026lt;CONTAINER ID or NAMES\u0026gt; Every time you restart your container, you are supposed to start all those services again before any HDFS related operations.\n(5) Destroy instance If you want to permanently remove container\n$ docker rm \u0026lt;CONTAINER ID or NAMES\u0026gt; (6) Destroy images If you want to permanently remove images\nList images first\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE sunlab/bigbox latest bfd258e00de3 16 hours ago 2.65GB Remove them by REPOSITORY or IMAGE ID using command:\n$ docker rmi \u0026lt;REPOSITORY or IMAGE ID\u0026gt; (7) Update images $ docker pull sunlab/bigbox (8) More official documents Please refer to this link for the introduction of images, containers, and storage drivers.\n(9) Optional: use docker-compose Docker Compose is a tool for defining and running multi-container Docker applications. A simple docker-compose.yml could simplify the parameters, and make the life easier.\nPlease refer to this link for some further instruction.\n3. Logs and Diagnosis System Configurations ## cat /proc/meminfo | grep Mem ## Current Memory MemTotal: 8164680 kB ## Note: This value shoud no less than 4GB MemFree: 175524 kB MemAvailable: 5113340 kB ## cat /proc/cpuinfo | grep 'model name' | head -1 ## CPU Brand model name\t: Intel(R) Core(TM) i7-7920HQ CPU @ 3.10GHz ## cat /proc/cpuinfo | grep 'model name' | wc -l ## CPU Count 4 ## df -h ## List Current Hard Disk Usage Filesystem Size Used Avail Use% Mounted on overlay 32G 4.6G 26G 16% / tmpfs 64M 0 64M 0% /dev ... ## ps -ef ## List Current Running Process UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 pts/0 00:00:00 /tini -- /bin/bash root 7 1 0 01:38 pts/0 00:00:00 /bin/bash root 77 1 0 01:43 ? 00:00:00 /usr/sbin/sshd zookeep+ 136 1 0 01:43 ? 00:00:14 /usr/lib/jvm/java-openjdk/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/lib/zookeeper/bin/../build/classes:/ yarn 225 1 0 01:43 ? 00:00:13 /usr/lib/jvm/java/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-pr ... ## lsof -i:9530 ## Find the Process Listening to Some Specific Port COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 3165 zeppelin 189u IPv4 229945 0t0 TCP *:9530 (LISTEN) Logs  hadoop-hdfs \u0026ndash; /var/log/hadoop-hdfs/* hadoop-mapreduce \u0026ndash; /var/log/hadoop-mapreduce/* hadoop-yarn \u0026ndash; /var/log/hadoop-yarn/* hbase \u0026ndash; /var/log/hbase/* hive \u0026ndash; /var/log/hive/* spark \u0026ndash; /var/log/spark/* zookeeper \u0026ndash; /var/log/zookeeper/* zeppelin \u0026ndash; /usr/local/zeppelin/logs/*  Misc. User and Role [root@bootcamp1 /]# su hdfs ## This command is used to switch your current user to hdfs ## Note: switch user requires special permission ## You can not switch back using su root again bash-4.2$ whoami ## check current user hdfs bash-4.2$ exit ## role is a stack, you can quit your role from hdfs to root [root@bootcamp1 /]# [root@bootcamp1 /]# sudo -u hdfs whoami ## execute a command 'whoami' using user 'hdfs' hdfs [root@bootcamp1 /]# User hdfs is the super user in HDFS system. User root is the super user in Linux system.\n[root@bootcamp1 /]# sudo -u hdfs hdfs dfs -mkdir /tmp In this case, user root has no permission to write data in /, but it could ask user hdfs to process it.\nRelative Path and Absolute Path An absolute or full path points to the same location in a file system, regardless of the current working directory. To do that, it must include the root directory. wiki.\nWhen we are talking about /mnt/host, it always pointing to the path /mnt/host. However, if the path is not startswith \u0026ldquo;/\u0026quot;, it means to start from \u0026ldquo;current working path\u0026rdquo;.\nIn Linux system, you can get your \u0026ldquo;current working path\u0026rdquo; using command\n## pwd /root In HDFS system, the \u0026ldquo;current working path\u0026rdquo; would be /user/\u0026lt;your-name\u0026gt;.\nA relative path would be the result of cwd plus your string.\nWhen we are coding in hadoop, we may required to fill in a location pointing to the path of input files. The synopsis of this path is is\n[schema://]your-path\nAn HDFS path hdfs:///hw1/test.csv is combined by hdfs:// and /hw1/test.csv. There are 3 slashes over there. If you only filled 2 slashes over there (hdfs://hw1/test.csv), it is equal to hdfs:///user/root/hw1/test.csv, which may not be expected.\nDitto for file:///path/to/your/local/file.csv.\nOther Linux Commands This environment is based on CentOS 7. This course does not requires you have too much knowledge in Linux, but if you can use some basic commands, that would be better.\nIf you are interested, please refer to this link for a Unix/Linux Command Cheat Sheet.\n"});index.add({'id':8,'href':'/docs/sessions/','title':"Sessions",'content':""});index.add({'id':9,'href':'/docs/sessions/scala-basic/','title':"Scala Basic",'content':"Scala Basics Learning Objectives\n Learn how to work with Scala interactive shell. Understand var and val. Define variables, functions and classes, and make function calls Understand Simple Build Tool (SBT).   In this section we will briefly go through the essential knowledge about Scala. You will first learn how to work with Scala shell, then learn how to use variables, functions with examples. Finally, we give instructions about how to compile and run a standalone program using sbt.\nScala Shell You can open a Scala shell by typing scala. Or, you can use sbt by typing sbt console. The second approach will help you add your project source code and dependencies into class path, so that your functions or library functions will be available for you to try to in the interactive shell. But in this training, we will stick to Scala shell for simplicity.\nOnce starting the Scala shell you will see\n$ scala Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0). Type in expressions to have them evaluated. Type :help for more information. scala\u0026gt; You can type :quit to stop and quit the shell, but don\u0026rsquo;t do that now :-) Next you will learn some Scala operations in the shell with the following materials.\nVariables Declare val and var In Scala, there are two types of variable, immutable val and mutable var. Unlike some functional programming language that requires immutable variables, Scala allows existence of mutable variables but immutable is recommended as it is easier to verify the correctness of your program. Suppose you are still in the Scala interactive shell. Define an immutable variable as\nscala\u0026gt; val myInt = 1 + 1 myInt: Int = 2 scala\u0026gt; myInt = 3 where val is a keyword in scala that makes the variables immutable. If you reassign a value to myInt, error will be reported.\nscala\u0026gt; myInt = 3 \u0026lt;console\u0026gt;:8: error: reassignment to val myInt = 3 ^ scala\u0026gt; In interactive shell, it\u0026rsquo;s possible to redefine variable with same name. In Scala source code file, it\u0026rsquo;s not allowed.\nscala\u0026gt; val a = 1 a: Int = 1 scala\u0026gt; val a = 2 a: Int = 2   Instead, variables declared with var are mutable. Ideally, we try to use val instead of var if possible as a good practice of functional programming.\nYou may have concern that maybe too many immutable variables will be declared. Actually, with chained function calls, that situation is not the case for well organized code.  An example of mutable variable is\nscala\u0026gt; var myString = \u0026#34;Hello Big Data\u0026#34; myString: String = Hello Big Data scala\u0026gt; myString = \u0026#34;Hello Healthcare\u0026#34; myString: String = Hello Healthcare Type Scala may seem like a script language like JavaScript or Python, as variable type is not specified explicitly. In fact, Scala is a static type language and the compiler can implicitly infer the type in most cases. However, you can always specify a type as\nscala\u0026gt; val myDouble: Double = 3 myDouble: Double = 3.0 It is always encouraged to specify the type so unless the type is too obvious.\nBesides simple built-in variable types like Int, Double and String, you will also learn about List and Tuple in the training:\nscala\u0026gt; val myList: List[String] = List(\u0026#34;this\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;string\u0026#34;) myList: List[String] = List(this, is, a, list, of, string) scala\u0026gt; val myTuple: (Double, Double) = (1.0, 2.0) myTuple: (Double, Double) = (1.0,2.0) Here the List[String] is syntax of generics in Scala, which is same as C#. In the above example, List[String] means a List of String. Similarly, (Double, Double) means a two-field tuple type and both the 1st element and the 2nd element should be of type Double.\nFunctions You can define a function and invoke the function like\nscala\u0026gt; def triple(x: Int): Int = { x*3 } triple: (x: Int)Int scala\u0026gt; triple(2) res0: Int = 6 Where x: Int is parameter and its type, and the second Int is function return type. There\u0026rsquo;s not explicit return statement, but the result of last expresssion x*3 will be returned (similar to some other programming languages like Ruby). In this example, as there is only one expression and return type can be infered by the compiler, you may define the function as\ndef triple(x: Int) = x*3 Scala is object-oriented (OO), function calls on a class method are straightforward like most OO languages (e.g. Java, C#)\nscala\u0026gt; myString = \u0026#34;Hello Healthcare\u0026#34; myString: String = Hello Healthcare scala\u0026gt; myString.lastIndexOf(\u0026#34;Healthcare\u0026#34;) res1: Int = 6 If the function does not have parameters, you can even call it without parenthesis\nscala\u0026gt; val myInt = 2 scala\u0026gt; myInt.toString res2: String = 2 You can also define an anonymous function and pass it to variable like a lambda expression in some other languages such as Python:\nscala\u0026gt; val increaseOne = (x: Int) =\u0026gt; x + 1 increaseOne: Int =\u0026gt; Int = \u0026lt;function1\u0026gt; scala\u0026gt; increaseOne(3) res3: Int = 4 Anonymous function is very useful as it can be passed as a parameter to a function call\nscala\u0026gt; myList.foreach{item: String =\u0026gt; println(item)} this is a list of string where item: String =\u0026gt; println(item) is an anonymous function. This function call can be further simplified to\nscala\u0026gt; myList.foreach(println(_)) scala\u0026gt; myList.foreach(println) where _ represents first parameter of the anonymous function with body println(_). Additional _ can be specified to represent more than one parameter. For example, we can calculate the total payment that a patient made by\nscala\u0026gt; val payments = List(1, 2, 3, 4, 5, 6) payments: List[Int] = List(1, 2, 3, 4, 5, 6) scala\u0026gt; payments.reduce(_ + _) res0: Int = 21 In above example, reduce will aggregate List[V] into V and we defined the aggregator as _ + _ to sum them up. Of course, you can write that more explicitly like\nscala\u0026gt; payments.reduce((a, b) =\u0026gt; a+b) res1: Int = 21 Here reduce is a construct from functional programming. It can be illustrated with the figure below where a function f is applied to one element at a time and the result together with next element will be parameters of the next function call until the end of the list.\nIt\u0026rsquo;s important to remember that for reduce operation, the input is List[V] and the output is V. Interested reader can learn more from wiki. In contrast to reduce, you can of course write code using for loop, which is verbose and very rare in Scala,\nscala\u0026gt; var totalPayment = 0 totalPayment: Int = 0 scala\u0026gt; for (payment \u0026lt;- payments) { totalPayment += payment } scala\u0026gt; totalPayment res2: Int = 21 scala\u0026gt; Class Declaration of a class in Scala is as simple as\nscala\u0026gt; class Patient(val name: String, val id: Int) defined class Patient scala\u0026gt; val patient = new Patient(\u0026#34;Bob\u0026#34;, 1) patient: Patient = Patient@755f5e80 scala\u0026gt; patient.name res13: String = Bob Here we see the succinct syntax of Scala again. class Patient(val name: String, val id: Int) not only defines constructor of Patient but also defines two member variables (name and id).\nA special kind of class that we will use a lot is Case Class. Case Class can be declared as\nscala\u0026gt; case class Patient(val name: String, val id: Int) and see below Pattern Matching for use case.\nPattern Matching You may know the switch..case in other languages. Scala provides a more flexible and powerful technique, Pattern Matching. Below example shows one can match by-value and by-type in one match.\nval payment:Any = 21 payment match { case p: String =\u0026gt; println(\u0026#34;payment is a String\u0026#34;) case p: Int if p \u0026gt; 30 =\u0026gt; println(\u0026#34;payment \u0026gt; 30\u0026#34;) case p: Int if p == 0 =\u0026gt; println(\u0026#34;zero payment\u0026#34;) case _ =\u0026gt; println(\u0026#34;otherwise\u0026#34;) } It\u0026rsquo;s very convenient to use case class in pattern matching\nscala\u0026gt; val p = new Patient(\u0026#34;Abc\u0026#34;, 1) p: Patient = Patient(Abc,1) scala\u0026gt; p match {case Patient(\u0026#34;Abc\u0026#34;, id) =\u0026gt; println(s\u0026#34;matching id is $id\u0026#34;)} matching id is 1 Here we not only matched p as Patient type, but also matched patient name and extracted one member field from the Patient class instance.\np match { case Patient(\u0026#34;Abc\u0026#34;, id) =\u0026gt; println(s\u0026#34;matching id is $id\u0026#34;) case _ =\u0026gt; println(\u0026#34;not matched\u0026#34;) } Standalone Program Working with large real-world applications, you usually need to compile and package your source code with some tools. Here we show how to compile and run a simple program with sbt. Run the sample code in \u0026lsquo;hello-bigdata\u0026rsquo; folder\n% cd ~/bigdata-bootcamp/sample/hello-bigdata % sbt run Attempting to fetch sbt ######################################################################### 100.0% Launching sbt from sbt-launch-0.13.8.jar [info] ..... [info] Done updating. [info] Compiling 1 Scala source to ./hello-bigdata/target/scala-2.10/classes... [info] Running Hello Hello bigdata [success] Total time: 2 s, completed May 3, 2015 8:42:48 PM The source code file hello.scala is compiled and invoked.\nFurther Reading This is a very brief overview of important features from the Scala language. We highly recommend readers to checkout references below to get a better and more complete understanding of Scala programming language.\n Twitter Scala School Official Scala Tutorial SBT tutorial  "});index.add({'id':10,'href':'/docs/sessions/scala-intro/','title':"Scala Intro",'content':"Scala Introduction Learning Objectives\n Provide more details of scala language   Basic Gramma Start using Scala After installed scala, you can type scala in command line and get result as follow:\n$ scala\rWelcome to Scala version 2.11.7\rType in expressions to have them evaluated.\rType :help for more information.\rscala\u0026gt; println(\u0026#34;Hello, World!\u0026#34;)\rHello, World!\rThe synopsis of a varialbe is:\nscala\u0026gt; val i:String = \u0026#34;abc\u0026#34;\ri: String = abc\r val means it it is immutable variable, you can use \u0026ldquo;var\u0026rdquo; to define a mutable variable i is the name of this variable String is the type of this string, it can be omitted here \u0026ldquo;abc\u0026rdquo; is the value of this variable  Define a function:\nscala\u0026gt; def foo(v0:Int, v1:Int):Int = {\r| println(v0 max v1)\r| v0 + v1\r| }\rfoo: (v0: Int, v1: Int)Int\rscala\u0026gt; foo(1, 2)\r2\rres0: Int = 3\rDefine a class:\nscala\u0026gt; class Foo(a:String, b:Int) {\r| def length = a.length\r| }\rdefined class Foo\rscala\u0026gt; val foo:Foo = new Foo(\u0026#34;Hello, World!\u0026#34;, 3)\rfoo: Foo = Foo@6438a396\rscala\u0026gt; println(foo.length)\r13\rDefine a case class:\nscala\u0026gt; case class Foo(a:String, b:Int)\rdefined class Foo\rscala\u0026gt; val foo:Foo = Foo(a = \u0026#34;Hello, World!\u0026#34;, b = 3)\rfoo: Foo = Foo(Hello, World!,3)\rscala\u0026gt; println(foo.a)\rHello, World!\rDifferences between case class and class\nDefine an Object\nscala\u0026gt; object Foo {\r| def greeting() {\r| println(\u0026#34;Greeting from Foo\u0026#34;)\r| }\r| }\rdefined object Foo\rscala\u0026gt; Foo.greeting()\rGreeting from Foo\rFunctions/variables in Object is similar to the static function and variable in Java.\nWhat is ought to be highligted is the use of \u0026ldquo;apply\u0026rdquo;. SomeObject.apply(v:Int) equals SomeObject(v:Int)\nscala\u0026gt; :paste\r// Entering paste mode (ctrl-D to finish)\r\rcase class Foo(a:String, b:Int)\robject Foo {\rdef apply(a:String): Foo =\rFoo(a, a.length)\r}\r// Exiting paste mode, now interpreting.\r\rdefined class Foo\rdefined object Foo\rscala\u0026gt; val foo = Foo(\u0026#34;Hello, World!\u0026#34;)\rfoo: Foo = Foo(Hello, World!,13)\rFinally, we should know the usage of code block.\nWe can create a code block anywhere, and the last line is the result of this block.\nFor example,\ndef foo(i:Int) = {\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r}\rval newList = List[Int](1, 2, 3).map(i =\u0026gt; foo(i))\rWe can use the follow lines instead:\nval newList = List[Int](1, 2, 3).map(i =\u0026gt; {\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r})\rA better practice here is:\nval newList = List[Int](1, 2, 3).map{i =\u0026gt;\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r}\rCase Study of some Common Types Option, Some, None We can use null in Scala as null pointer, but it is not recommended. We are supposed to use Option[SomeType] to indicate this variable is optional.\nWe can assueme every variable without Option are not null pointer if we are not calling java code. This help us reduce a lot of code.\nIf we wanna to get a variable with Option, here are two method\nval oi = Option(1)\rval i = oi match {\rcase Some(ri) =\u0026gt; ri\rcase None =\u0026gt; -1\r}\rprintln(i)\rBesides, we can also use method \u0026ldquo;isDefined/isEmpty\u0026rdquo;.\nval oi = Option(1)\rif(oi.isDefined) {\rprintln(s\u0026#34;oi: ${oi.get}\u0026#34;)\r} else {\rprintln(\u0026#34;oi is empty\u0026#34;)\r}\rWhat should be highlighted is Option(null) returns None, but Some(null) is Some(null) which is not equals None.\nmatch is a useful reserved words, we can use it in various of situations\nFirstly, we can use it as \u0026ldquo;switch\u0026rdquo; \u0026amp; \u0026ldquo;case\u0026rdquo; in some other programming languages.\ntrue match {\rcase true =\u0026gt; println(\u0026#34;true\u0026#34;)\rcase false =\u0026gt; println(\u0026#34;false\u0026#34;)\r}\rSecondly, we can find it by the type of the data,\ncase class A(i:Int,j:Double)\rcase class B(a:A, k:Double)\rval data = B(A(1,2.0),3.14)\rdata match {\rcase B(A(_,v),_) =\u0026gt;\rprintln(s\u0026#34;required value: $v\u0026#34;)\rcase _ =\u0026gt;\rprintln(\u0026#34;match failed\u0026#34;)\r}\rGiven a case class B, but we only wish to retrievee the value B.a.j, we can use \u0026ldquo;_\u0026rdquo; as placeholder.\nCommon methods in List, Array, Set, and so on In scala, we always transfer the List( Array, Set, Map etc.) from one status to another status. the methods of\n  toList, toArray, toSet \u0026ndash; convert each other\n  par \u0026ndash; Parallelize List, Array and Map, the result of Seq[Int]().par is ParSeq[Int], you will able to process each element in parallel when you are using foreach, map etc., and unable to call \u0026ldquo;sort\u0026rdquo; before you are using \u0026ldquo;toList\u0026rdquo;.\n  distinct \u0026ndash; Removes duplicate elements\n  foreach \u0026ndash; Process each element and return nothing\n  List[Int](1,2,3).foreach{ i =\u0026gt;\rprintln(i)\r}\rIt wil print 1, 2, 3 in order\nList[Int](1,2,3).par.foreach{ i =\u0026gt;\rprintln(i)\r}\rIt will print 1, 2, 3, but the order is not guaranteed.\n map \u0026ndash; Process each element and construct a List using return value  List[Int](1,2,3).map{ i =\u0026gt; i + 1}\rIt will return List[Int](2,3,4)\nThe result of List[A]().map(some-oper-return-type-B) is List[B], while the result of Array[A]().map map is Array[B].\n flatten \u0026ndash; The flatten method takes a list of lists and flattens it out to a single list:  scala\u0026gt; List[List[Int]](List(1,2),List(3,4)).flatten\rres1: List[Int] = List(1, 2, 3, 4)\rscala\u0026gt; List[Option[Integer]](Some(1),Some[Integer](null),Some(2),None,Some(3)).flatten\rres2: List[Integer] = List(1, null, 2, 3)\r  flatMap \u0026ndash; The flatMap is similar to map, but it takes a function returning a list of elements as its right operand. It applies the function to each list element and returns the concatenation of all function results. The result equals to map + flatten\n  collect \u0026ndash; The iterator obtained from applying the partial function to every element in it for which it is defined and collecting the results.\n  scala\u0026gt; List(1,2,3.4,\u0026#34;str\u0026#34;) collect {\r| case i:Int =\u0026gt; (i * 2).toString\r| case f:Double =\u0026gt; f.toString\r| }\rres0: List[String] = List(2, 4, 3.4)\rThe function match elements in Int and Double, process them and return the value, but ignore string elements.\n filter \u0026ndash; Filter this list  scala\u0026gt; List(1,2,3).filter(_ % 2 == 0)\rres1: List[Int] = List(2)\r filterNot \u0026ndash; Similar to filter  scala\u0026gt; List(1,2,3).filterNot(_ % 2 == 0)\rres2: List[Int] = List(1, 3)\r forall \u0026ndash; Return true if All elements are return true by the partial function. It will immediately return once one element returns false, and ignore the rest elements.  scala\u0026gt; List(2,1,0,-1).forall{ i =\u0026gt;\r| val res = i \u0026gt; 0\r| println(s\u0026#34;$i\u0026gt; 0? $res\u0026#34;)\r| res\r| }\r2 \u0026gt; 0? true\r1 \u0026gt; 0? true\r0 \u0026gt; 0? false\rres0: Boolean = false\r exists \u0026ndash; Return true if there are at least One element returns true.  scala\u0026gt; List(2,1,0,-1).exists{ i =\u0026gt;\r| val res = i \u0026lt;= 0\r| println(s\u0026#34;$i\u0026lt;= 0? $res\u0026#34;)\r| res\r| }\r2 \u0026lt;= 0? false\r1 \u0026lt;= 0? false\r0 \u0026lt;= 0? true\rres2: Boolean = true\r find \u0026ndash; Return the first element returns true by the partial function. Return None if no elemet found.  scala\u0026gt; List(2,1,0,-1).find{ i =\u0026gt;\r| val res = i \u0026lt;= 0\r| println(s\u0026#34;$i\u0026lt;= 0? $res\u0026#34;)\r| res\r| }\r2 \u0026lt;= 0? false\r1 \u0026lt;= 0? false\r0 \u0026lt;= 0? true\rres3: Option[Int] = Some(0)\r sortWith \u0026ndash; sort the elements  scala\u0026gt; List(1,3,2).sortWith((leftOne,rightOne) =\u0026gt; leftOne \u0026gt; rightOne)\rres5: List[Int] = List(3, 2, 1)\r zipWithIndex \u0026ndash;  List(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;).zipWithIndex.foreach{ kv:(String,Int) =\u0026gt; println(s\u0026#34;k:${kv._1}, v:${kv._2}\u0026#34;)}\rIt will rebuild a List with index\nk:a, v:0\rk:b, v:1\r for - Scala\u0026rsquo;s keyword for can be used in various of situations.  Basically,\nfor{\ri \u0026lt;- List(1,2,3)\r} yield (i,i+1)\rIt equals:\nList(1,2,3).map(i =\u0026gt; (i, i+1))\rBesides,\nfor{\ri \u0026lt;- List(1,2,3)\rj \u0026lt;- List(4,5,6)\r} yield (i,j)\rWe will get the cartesian product of List(1,2,3) and List(4,5,6): List((1,4), (1,5), (1,6), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6))\nWe can add a filter in condition:\nfor{\ri \u0026lt;- List(1,2,3)\rif i != 1\rj \u0026lt;- List(4,5,6)\rif i * j % 2 == 1\r} yield (i,j)\rthe result is : List((3,5))\nAnother usage of for is as follow:\nLet\u0026rsquo;s define variables as follow:\nval a = Some(1)\rval b = Some(2)\rval c = Some(3)\rWe can execute like this:\nfor {\ri \u0026lt;- a\rj \u0026lt;- b\rk \u0026lt;- c\rr \u0026lt;- {\rprintln(s\u0026#34;i: $i, j:$j, k:$k\u0026#34;)\rSome(i * j * k)\r}\r} yield r\rThe response is:\ni: 1, j:1, k:3\rres9: Option[Int] = Some(6)\rLet\u0026rsquo;s define b as None:\nscala\u0026gt; val b:Option[Int] = None\rb: Option[Int] = None\rscala\u0026gt; for {\r| i \u0026lt;- a\r| j \u0026lt;- b\r| k \u0026lt;- c\r| r \u0026lt;- {\r| println(s\u0026#34;i: $i, j:$j, k:$k\u0026#34;)\r| Some(i * j * k)\r| }\r| } yield r\rres14: Option[Int] = None\r while - Similar to while in java  var i = 0\rwhile ({\ri = i + 1\ri \u0026lt; 1000\r}){\r// body of while\r\tprintln(s\u0026#34;i: $i\u0026#34;)\r}\r  to, until — (1 to 10) will generate a Seq, with the content of (1,2,3,4…10), (0 until 10) will generate a sequence from 0 to 9. With some test, (0 until 1000).map(xxx) appears to be slower than var i=0; while( i \u0026lt; 1000) { i += 1; sth. else}, but if the body of map is pretty heavy, this cost can be ignored.\n  headOption - Get the head of one list, return None if this list is empty\n  head - Get the head of one list, throw exception if this list is empty\n  take \u0026ndash; Get first at most N elements. (from left to right)\n  scala\u0026gt; List(1,2,3).take(2)\rres0: List[Int] = List(1, 2)\rscala\u0026gt; List(1,2).take(3)\rres1: List[Int] = List(1, 2)\r drop \u0026ndash; Drop first at most N elements.  scala\u0026gt; List(1,2,3).drop(2)\rres2: List[Int] = List(3)\rscala\u0026gt; List(1,2).drop(3)\rres3: List[Int] = List()\rdropRight will drop elements from right to left.\n slice \u0026ndash; Return list in [start-offset, end-offset)  scala\u0026gt; List(1,2,3).slice(1,2)\rres7: List[Int] = List(2)\rscala\u0026gt; List(1,2,3).slice(2,2)\rres8: List[Int] = List()\rval offset = 1\rval size = 3\rList(1,2,3,4,5).slice(offset, size + offset)\rIf the end-offset is greater than the length of this list, it will not throw exception.\n splitAt \u0026ndash; Split this list into two from offset i  scala\u0026gt; List(1,2,3).splitAt(1)\rres10: (List[Int], List[Int]) = (List(1),List(2, 3))\r groupBy \u0026ndash; Partitions a list into a map of collections according to a discriminator function  scala\u0026gt; List(1,2,3).groupBy(i =\u0026gt; if(i % 2 == 0) \u0026#34;even\u0026#34; else \u0026#34;odd\u0026#34; )\rres11: scala.collection.immutable.Map[String,List[Int]] = Map(odd -\u0026gt; List(1, 3), even -\u0026gt; List(2))\r partition \u0026ndash; Splits a list into a pair of collections; one with elements that satisfy the predicate, the other with elements that do not, giving the pair of collections (xs filter p, xs.filterNot p).  scala\u0026gt; List(1,2,3).partition(_ % 2 == 0)\rres12: (List[Int], List[Int]) = (List(2),List(1, 3))\r grouped \u0026ndash; The grouped method chunks its elements into increments.  scala\u0026gt; List(1,2,3,4,5).grouped(2)\rres13: Iterator[List[Int]] = Iterator(List(1, 2), List(3, 4), List(5))\rYou can visit this PDF for an official guide.\nWe also highly recomended to read the book Programming in Scala for more detail instruction.\n"});index.add({'id':11,'href':'/docs/sessions/spark-application/','title':"Spark Application",'content':"Spark Application Learning Objectives\n Prepare data for machine learning applications. Save/load constructed data to external storage.   In this section, we will show how to prepare suitable data for building predictive models to predict heart failure (HF). We will first briefly introduce data types involved. Then we show how to construct training/testing samples from the input data using Spark. Finally we will export data in suitable format for modeling later.\nData Types For many machine learning tasks, such as classification, regression, and clustering, a data point is often represented as a feature vector. Each coordinate of the vector corresponds to a particular feature of the data point.\nFeature Vector MLlib, the machine learning module of Spark, supports two types of vectors: dense and sparse. A dense vector is basically a Double array of length equals to the dimension of the vector. If a vector contains only a few non-zero entries, we can then more efficiently represent the vector by a sparse vector with non-zero indices and the corresponding values only. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\nThe base class of a vector is Vector, and there are two implementations: DenseVector and SparseVector. We recommend using the factory methods implemented in Vectors to create vectors.\nscala\u0026gt; import org.apache.spark.mllib.linalg.{Vector, Vectors} // Create a dense vector (1.0, 0.0, 3.0). scala\u0026gt; val dv = Vectors.dense(1.0, 0.0, 3.0) // Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries. scala\u0026gt; val sv = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) Labeled Point A labeled point is a vector, either dense or sparse, associated with a label/prediction target. In Spark MLlib, labeled points are used as input to supervised learning algorithms. For example, in binary classification like HF prediction, a label should be either 0 or 1. For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, \u0026hellip;. For regression problem like payment prediction, a label is a real-valued number.\nscala\u0026gt; import org.apache.spark.mllib.linalg.Vectors scala\u0026gt; import org.apache.spark.mllib.regression.LabeledPoint // Create a labeled point with label 1 and a dense feature vector. scala\u0026gt; val labeled1 = LabeledPoint(1, Vectors.dense(1.0, 0.0, 3.0)) // Create a labeled point with label 0 and a sparse feature vector. scala\u0026gt; val labeled0 = LabeledPoint(0, Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))) Feature Construction Overview To apply machine learning algorithms, we need to transform our data into RDD[LabeledPoint]. This feature construction is similar to what we did in Hadoop Pig, but will be more concise since we are programming in Scala on Spark. We will need to consider an one-year prediction window. Specifically, we will only use data one year before HF diagnosis. The figure below depicts relationship between prediction window and target.\nWe can also specify an observation window, inside which data will be used to construct feature vectors.\nHigh level steps are depicted as the figure below\nOur parallelization will be on patient level, i.e. each element in RDD is everything about exactly one patient. Feature and prediction target for each patient is almost independent from the others. Recall that our data file is in the following form:\n00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 .... Each line is a 4-tuple (patient-id, event-id, timestamp, value). Suppose now our goal is to predict if a patient will have heart failure. We can use the value associated with the event heartfailure as the label. This value can be either 1.0 (the patient has heart failure) or 0.0 (the patient does not have heart failure). We call a patient with heart failure a positive example or case patient, and a patient without heart failure a negative example or control patient. For example, in the above snippet we can see that patient 00013D2EFD8E45D1 is a positive example. The file case.csv consists of only positive examples, and the file control.csv consists of only negative examples.\nWe will use the values associated with events other than heartfailure to construct feature vector for each patient. Specifically, the length of the feature vector is the number of distinct event-id's, and each coordinate of the vector stores the aggregated value corresponds to a particular event-id. The values associated with events not shown in the file are assume to be 0. Since each patient typically has only a few hundreds of records (lines) compared to thousands of distinct events, it is more efficient to use SparseVector. Note that each patient can have multiple records with the same event-id. In this case we sum up the values associated with a same event-id as feature value and use event-id as feature name.\n1. Load data The file input/case.csv consists of only positive examples, and the file input/control.csv consists of only negative examples. We will load them together. Since the data will be used more than once, we use cache() to prevent reading in the file multiple times.\ncase class Event(patientId: String, eventId: String, timestamp: Int, value: Double) val rawData = sc.textFile(\u0026#34;input/\u0026#34;). map{line =\u0026gt; val splits = line.split(\u0026#34;,\u0026#34;) new Event(splits(0), splits(1), splits(2).toInt, splits(3).toDouble) } 2. Group patient data One patient\u0026rsquo;s index date, prediction target etc are independent from another patient, so that we can group by patient-id to put everything about one patient together. When we run map operation, Spark will help us parallelize computation on patient level.\n// group raw data with patient id and ignore patient id // then we will run parallel on patient lelvel val grpPatients = rawData.groupBy(_.patientId).map(_._2) The groupBy operation can be illustrated with the example below\nPlease recall that _._2 will return second field of a tuple. In this case it will return the List[event] for a given patient. Finally the grpPatients will be RDD[List[event]]\n3. Define target and feature values Now, we can practice our patient level parallelization. For each patient, we first find the prediction target, which is encoded into an event with name heartfailure, then we identify the index date and keep only useful events before the index date for feature construction. In feature construction, we aggregate the event value into features using sum function and use the event name as the feature name.\nval patientTargetAndFeatures = grpPatients.map{events =\u0026gt; // find event that encode our prediction target, heart failure  val targetEvent = events.find(_.eventId == \u0026#34;heartfailure\u0026#34;).get val target = targetEvent.value // filter out other events to construct features  val featureEvents = events.filter(_.eventId != \u0026#34;heartfailure\u0026#34;) // define index date as one year before target  // and use events happened one year before heart failure  val indexDate = targetEvent.timestamp - 365 val filteredFeatureEvents = featureEvents.filter(_.timestamp \u0026lt;= indexDate) // aggregate events into features  val features = filteredFeatureEvents. groupBy(_.eventId). map{case(eventId, grpEvents) =\u0026gt; // user event id as feature name  val featureName = eventId // event value sum as feature value  val featureValue = grpEvents.map(_.value).sum (featureName, featureValue) } (target, features) } The construction of target is relatively simple, but the process of constructing features is tricky. The example below show what happened in main body of above map operation to illustrate how features were constructed\nOur final filteredFeatureEvents should be RDD[(target, Map[feature-name, feature-value])] and we can verify that by the following:\nscala\u0026gt; patientTargetAndFeatures.take(1) res0: Array[(Double, scala.collection.immutable.Map[String,Double])] = Array((0.0,Map(DRUG36987241603 -\u0026gt; 60.0, DRUG00378181701 -\u0026gt; 30.0, DRUG11517316909 -\u0026gt; 20.0, DRUG53002055230 -\u0026gt; 200.0, DRUG23490063206 -\u0026gt; 30.0, DRUG61113074382 -\u0026gt; 60.0, DRUG58016093000 -\u0026gt; 60.0, DRUG52604508802 -\u0026gt; 30.0, DRUG58016037228 -\u0026gt; 10.0, DRUG60491080134 -\u0026gt; 30.0, DRUG51079093119 -\u0026gt; 360.0, DRUG00228275711 -\u0026gt; 30.0, DRUG63629290803 -\u0026gt; 120.0, DIAG4011 -\u0026gt; 1.0, DRUG58016075212 -\u0026gt; 90.0, DRUG00378412401 -\u0026gt; 30.0, DRUG63629260701 -\u0026gt; 30.0, DRUG00839619116 -\u0026gt; 30.0, DRUG11390002315 -\u0026gt; 30.0, DRUG58016058050 -\u0026gt; 60.0, DRUG55289082930 -\u0026gt; 60.0, DRUG36987154502 -\u0026gt; 30.0, DRUG00364095301 -\u0026gt; 30.0, DRUG58016021326 -\u0026gt; 180.0, DRUG54868593401 -\u0026gt; 30.0, DRUG58016054035 -\u0026gt; 30.0, DRUG64464000105 -\u0026gt; 30.0, DRUG58016076573 -\u0026gt; 30.0, DRUG00839710006... 4. Feature name to id In the previous step, we computed filteredFeatureEvents as RDD[(target, Map[feature-name, feature-value])]. In order to convert feature-name to some integer id as required by most machine learning modules including MLlib, we will need to collect all unique feauture names and associate them with integer ids.\n// assign a unique integer id to feature name val featureMap = patientTargetAndFeatures. // RDD[(target, Map[feature-name, feature-value])]  flatMap(_._2.keys). // get all feature names  distinct. // remove duplication  collect. // collect to driver program  zipWithIndex. // assign an integer id  toMap // convert to Map[feature-name, feature-id] Here we used an operation named flatMap. Below is an example, and we can think of flatMap as a two step operation, map and flatten. As a result, patientTargetAndFeatures.flatMap(_._2.keys) will give RDD[feature-name].\nNext we visualize the steps after flatMap:\nHere collect is not depicted but what collect does is to collect data from distributed to centralized storage on the driver. Here we assume the resulting data matrix is not too big. If the data matrix is very big, alternative approach may be required such as join. Note that many the common functions like zipWithIndex have the same name on RDD and on common local data structures like List.\nIf you get confused about result of certain operations, you can avoid chain of operation calls and instead print out the result of each step.  5. Create final LabeledPoint In this last step, we transform (target, features) for each patient into LabeledPoint. Basically, we just need to translate feature name in features into feautre id and create a feature vector then associate the vector with target.\n// broadcast feature map from driver to all workers val scFeatureMap = sc.broadcast(featureMap) val finalSamples = patientTargetAndFeatures.map {case(target, features) =\u0026gt; val numFeature = scFeatureMap.value.size val indexedFeatures = features. toList. // map feature name to id to get List[(feature-id, feature-value)]  map{case(featureName, featureValue) =\u0026gt; (scFeatureMap.value(featureName), featureValue)} val featureVector = Vectors.sparse(numFeature, indexedFeatures) val labeledPoint = LabeledPoint(target, featureVector) labeledPoint } Here in above example, we called sc.broadcast. As indicated by its name, this function is used for broadcasting data from driver to workers so that workers will not need to copy on demand and waste bandwidth thus slow down the process. Its usage is very simple, call val broadcasted = sc.broadcast(object) and use broadcasted.value to access original object. Please be aware of the fact that such broadcasted object is read-only.\nSave With data readily available as RDD[LabeledPoint], we can save it into a common format accepted by a lot of machine learning modules, the LibSVM/svmlight format, named after LibSVM/svmlight package.\nimport org.apache.spark.mllib.util.MLUtils MLUtils.saveAsLibSVMFile(finalSamples, \u0026#34;samples\u0026#34;) You can achieve this by\nval mapping = featureMap. toList. sortBy(_._2). map(pair =\u0026gt; s\u0026#34;${pair._1}|${pair._2}\u0026#34;). // intentionally use special seperator  mkString(\u0026#34;\\n\u0026#34;) scala.tools.nsc.io.File(\u0026#34;mapping.txt\u0026#34;).writeAll(mapping) "});index.add({'id':12,'href':'/docs/sessions/spark-basic/','title':"Spark Basic",'content':"Spark Basics Learning Objectives\n Invoke command in Spark interactive shell. Be familiar with RDD concept. Know basic RDD operations.   Spark Shell Spark can run in several modes, including YARN client/server, Standalone, Mesos and Local. For this training, we will use local mode. Specifically, you can start the Spark interactive shell by invoking the command below in the terminal to run Spark in the local mode with two threads. Then you will see\n\u0026gt; spark-shell --master \u0026quot;local[2]\u0026quot; --driver-memory 6G Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties ... [messages] ... Spark context available as sc. scala\u0026gt; Here you can set --driver-memory according to your local setting. If your setting of driver memory is larger than the VM memory, don\u0026rsquo;t forget to change the VM memory setting first.\nIn Spark, we call the main entrance of a Spark program the driver and Spark distribute computation to workers to compute. Here in the interactive shell, the Spark shell program is the driver. In above example we set the memory of driver program to 3GB as in local mode driver and worker are together. A driver program can access Spark through a SparkContext object, which represents a connection to a computing cluster. In the above interactive shell, SparkContext is already created for you as variable sc. You can input sc to see its type.\nscala\u0026gt; sc res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@27896d3b You may find the logging statements that get printed in the shell distracting. You can control the verbosity of the logging. To do this, you can create a file in the conf directory called log4j.properties. The Spark developers already include a template for this file called log4j.properties.template. To make the logging less verbose, make a copy of conf/log4j.properties.template called conf/log4j.properties and find the following line:\nlog4j.rootCategory=INFO, console Replace INFO with WARN so that only WARN messages and above are shown.\nRDD Resilient Distributed Dataset (RDD) is Spark\u0026rsquo;s core abstraction for working with data. An RDD is simply a fault-tolerant distributed collection of elements. You can imagine RDD as a large array but you cannot access elements randomly but you can apply the same operations to all elements in the array easily. In Spark, all the work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute results. There are two ways to create RDDs: by distributing a collection of objects (e.g., a list or set), or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\nParallelized Collections For the demo purpose, the simplest way to create an RDD is to take an existing collection (e.g. a Scala Array) in your program and pass it to SparkContext\u0026rsquo;s parallelize() method.\nscala\u0026gt; val data = Array(1, 2, 3, 4, 5) data: Array[Int] = Array(1, 2, 3, 4, 5) scala\u0026gt; val distData = sc.parallelize(data) distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at \u0026lt;console\u0026gt;:23 Once created, the distributed dataset (distData) can be operated in parallel. For example, we can add up the elements by calling distData.reduce((a, b) =\u0026gt; a + b). You will see more operations on RDD later on.\nParallelizing a collection is useful when you are learning Spark. However, this is not encouraged in production since it requires the entire dataset to be in memory of the driver\u0026rsquo;s machine first. Instead, importing data from external datasets should be employed.  External Datasets A common way for creating RDDs is loading data from external storage. Below you will learn how to load data from a file system. Assuming you have put some data into HDFS as described in the HDFS Basic section. If not, please do that first.\nscala\u0026gt; val lines = sc.textFile(\u0026#34;input/case.csv\u0026#34;) lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at \u0026lt;console\u0026gt;:21 Here in the above example, each line of the original file will become an element in the lines RDD.\nReading data from a file system, Spark relies on the HDFS library. In above example we assume HDFS is well configured through environmental variables or configuration files so that data is ready in HDFS.  RDD Operations RDDs offer two types of operations: transformations and actions:\n Transformations are operations on RDDs that return a new RDD, such as map() and filter(). Actions are operations that return a result to the driver program or write it to storage, such as first() and count().  Spark treats transformations and actions very differently, so understanding which type of operation you are performing is very important. You can check whether a function is a transformation or an action by looking at its return type: transformations return RDDs, whereas actions return some other data type.\nAll transformations in Spark are lazy, in that they do not compute the results right away. Instead, they just remember the operations applied to some base dataset (e.g. an Array or a file). The transformations are only computed when an action requires a result to be returned to the driver program. Therefore, the above command of reading in a file has not actually been executed yet. We can force the evaluation of RDDs by calling any actions.\nLet\u0026rsquo;s go through some common RDD operations using the healthcare dataset. Recall that in the file case.csv, each line is a 4-field tuple (patient-id, event-id, timestamp, value).\nCount In order to know how large is our raw event sequence data, we can count the number of lines in the input file using count operation, i.e.\nscala\u0026gt; lines.count() res1: Long = 14046 Clearly, count is an action.\nTake You may wonder what the loaded data looks like, you can take a peek at the data. The take(k) will return the first k elements in the RDD. Spark also provides collect() which brings all the elements in the RDD back to the driver program. Note that collect() should only be used when the data is small. Both take and collect are actions.\nscala\u0026gt; lines.take(5) res2: Array[String] = Array(00013D2EFD8E45D1,DIAG78820,1166,1.0, 00013D2EFD8E45D1,DIAGV4501,1166,1.0, 00013D2EFD8E45D1,heartfailure,1166,1.0, 00013D2EFD8E45D1,DIAG2720,1166,1.0, 00013D2EFD8E45D1,DIAG4019,1166,1.0) We got the first 5 records in this RDD. However, this is hard to read due to a poor format. We can make it more readable by traversing the array to print each record on its own line.\nscala\u0026gt; lines.take(5).foreach(println) 00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 00013D2EFD8E45D1,DIAG4019,1166,1.0 Note that in above 3 code block examples, the RDD lines has been computed (i.e. read in from file) 3 times. We can prevent this by calling lines.cache(), which will cache the RDD in memory to avoid reloading.\nscala\u0026gt; lines.take(5).map(_.split(\u0026#34;,\u0026#34;)).map(_(1)).foreach(println) Map The map operation in Spark is similar to that of Hadoop. It\u0026rsquo;s a transformation that transforms each item in the RDD into a new item by applying the provided function. Notice this map will map exactly one element from source to target. For example, suppose we are only interested in knowing IDs of patients, we use map like\nscala\u0026gt; lines.map(line =\u0026gt; line.split(\u0026#34;,\u0026#34;)(0)) It is also possible to write a more complex, multiple-lines map function. In this case, curly braces should be used in place of parentheses. For example, we can get both patient-id and event-id as a tuple at the same time.\nscala\u0026gt; lines.map{line =\u0026gt; val s = line.split(\u0026#34;,\u0026#34;) (s(0), s(1)) } Filter As indicated by its name, filter can transform an RDD to another RDD by keeping only elements that satisfy the filtering condition. For example, we want to count the number of events collected for a particular patient to verify amount of the data from that patient. We can use a filter function.\nscala\u0026gt; lines.filter(line =\u0026gt; line.contains(\u0026#34;00013D2EFD8E45D1\u0026#34;)).count() res4: Long = 200 Distinct distinct is a transformation that transforms a RDD to another by eliminating duplications. We can use that to count the number of distinct patients. In order to do this, we first extract the patient ID from each line. We use the map() function as described above. In this example, we transform each line into the corresponding patient ID by extracting only the first column. We then eliminate duplicate IDs by the distinct() function.\nscala\u0026gt; lines.map(line =\u0026gt; line.split(\u0026#34;,\u0026#34;)(0)).distinct().count() res5: Long = 100 Group Sometimes, you will need to group the input events according to patient-id to put everything about each patient together. For example, in order to extract index date for predictive modeling, you may first group input data by patient then handle each patient seperately in parallel. We can see each element in RDD is a (Key, Value) pair (patient-id, iterable[event]).\n\u0026gt; val patientIdEventPair = lines.map{line =\u0026gt; val patientId = line.split(\u0026#34;,\u0026#34;)(0) (patientId, line) } \u0026gt; val groupedPatientData = patientIdEventPair.groupByKey \u0026gt; groupedPatientData.take(1) res1: Array[(String, Iterable[String])] = Array((0102353632C5E0D0,CompactBuffer(0102353632C5E0D0,DIAG29181,562,1.0, 0102353632C5E0D0,DIAG29212,562,1.0, 0102353632C5E0D0,DIAG34590,562,1.0, 0102353632C5E0D0,DIAG30000,562,1.0, 0102353632C5E0D0,DIAG2920,562,1.0, 0102353632C5E0D0,DIAG412,562,1.0, 0102353632C5E0D0,DIAG28800,562,1.0, 0102353632C5E0D0,DIAG30391,562,1.0, 0102353632C5E0D0,DIAGRG894,562,1.0, 0102353632C5E0D0,PAYMENT,562,6000.0, 0102353632C5E0D0,DIAG5781,570,1.0, 0102353632C5E0D0,DIAG53010,570,1.0, 0102353632C5E0D0,DIAGE8490,570,1.0, 0102353632C5E0D0,DIAG27651,570,1.0, 0102353632C5E0D0,DIAG78559,570,1.0, 0102353632C5E0D0,DIAG56210,570,1.0, 0102353632C5E0D0,DIAG5856,570,1.0, 0102353632C5E0D0,heartfailure,570,1.0, 0102353632C5E0D0,DIAG5070,570,1.0, 0102353632C5E0D0,DIAGRG346,570,1.0,... .... Reduce By Key reduceByKey transforms an RDD[(K, V)] into RDD[(K, List[V])] (like what groupByKey does) and then apply reduce function on List[V] to get final output RDD[(K, V)]. Please be careful that we intentionally denote V as return type of reduce which should be same as input type of the list element. Suppose now we want to calculate the total payment by each patient. A payment record in the dataset is in the form of (patient-id, PAYMENT, timestamp, value).\nval payment_events = lines.filter(line =\u0026gt; line.contains(\u0026#34;PAYMENT\u0026#34;)) val payments = payment_events.map{ x =\u0026gt; val s = x.split(\u0026#34;,\u0026#34;) (s(0), s(3).toFloat) } val paymentPerPatient = payments.reduceByKey(_+_) The payment_events RDD returned by filter contains those records associated with payment. Each item is then transformed to a key-value pair (patient-id, payment) with map. Because each patient can have multiple payments, we need to use reduceByKey to sum up the payments for each patient. Here in this example, patient-id will be served as the key, and payment will be the value to sum up for each patient. The figure below shows the process of reduceByKey in our example\nSort We can then find the top-3 patients with the highest payment by using sortBy first.\nscala\u0026gt; paymentPerPatient.sortBy(_._2, false).take(3).foreach(println) and output is\n(0085B4F55FFA358D,139880.0) (019E4729585EF3DD,108980.0) (01AC552BE839AB2B,108530.0) Again in sortBy we use the _ placeholder, so that _._2 is an anonymous function that returns the second element of a tuple, which is the total payment a patient. The second parameter of sortBy controls the order of sorting. In above example, false means decreasing order.\nscala\u0026gt; val maxPaymentPerPatient = payments.reduceByKey(math.max) Here, reduceByKey(math.max) is the simplified expression of reduceByKey(math.max(_,_)) or reduceByKey((a,b) =\u0026gt; math.max(a,b)). math.max is a function in scala that turns the larger one of two parameters.\n\u0026lt;ExerciseComponent question=\u0026quot;Count the number of records for each drug (event-id starts with \u0026ldquo;DRUG\u0026rdquo;)\u0026rdquo; answer=\u0026quot;\u0026ldquo;\u0026gt;\nscala\u0026gt; val drugFrequency = lines.filter(_.contains(\u0026#34;DRUG\u0026#34;)). map{ x =\u0026gt; val s = x.split(\u0026#34;,\u0026#34;) (s(1), 1) }.reduceByKey(_+_) Statistics Now we have total payment information of patients, we can run some basic statistics. For RDD consists of numeric values, Spark provides some useful statistical primitives.\nscala\u0026gt; val payment_values = paymentPerPatient.map(payment =\u0026gt; payment._2).cache() scala\u0026gt; payment_values.max() res6: Float = 139880.0 scala\u0026gt; payment_values.min() res7: Float = 3910.0 scala\u0026gt; payment_values.sum() res8: Double = 2842480.0 scala\u0026gt; payment_values.mean() res9: Double = 28424.8 scala\u0026gt; payment_values.stdev() res10: Double = 26337.091771112468 Set Operation RDDs support many of the set operations, such as union and intersection, even when the RDDs themselves are not properly sets. For example, we can combine the two files by the union function. Please notice that union here is not strictly identical to union operation in mathematics as Spark will not remove duplications.\nscala\u0026gt; val linesControl = sc.textFile(\u0026#34;input/control.csv\u0026#34;) scala\u0026gt; lines.union(linesControl).count() res11: Long = 31144 Here, a more straightforward way is to use directory name to read in multiple files of that directory into a single RDD.\nscala\u0026gt; val lines = sc.textFile(\u0026#34;input/\u0026#34;)   scala\u0026gt; val drugCase = sc.textFile(\u0026#34;input/case.csv\u0026#34;). filter(_.contains(\u0026#34;DRUG\u0026#34;)). map(_.split(\u0026#34;,\u0026#34;)(1)). distinct() scala\u0026gt; val drugControl = sc.textFile(\u0026#34;input/control.csv\u0026#34;). filter(_.contains(\u0026#34;DRUG\u0026#34;)). map(_.split(\u0026#34;,\u0026#34;)(1)). distinct() scala\u0026gt; drugCase.intersection(drugControl).count() res: Long = 396 Datasets  This section is work in progress Dataset is added from Spark 1.6+   A Dataset is a new interface added from Spark 1.6 that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.\nA Dataset has a concrete type of a Scala primitive type (Integer, Long, Boolean, etc) or a subclass of a Product - a case class. The case class is preferred for Spark because it handles the serialization code for you thus allowing Spark to shuffle data between workers. This can additional be handled implementing Externalizable which is a much more efficient mechanism to handle serialization, or by using a compact serializer like Kryos.\nAdditionally, Spark no longer uses SparkContext directly but prefers the use of a SparkSession that encapsulates a SparkContext and a SqlContext. The SparkSession is a member of the sql package.\nThere is a wealth of great documentation on the Spark development site.\nCreation of Datasets Datasets can be created explicitly or loaded form a source (e.g. file, stream, parquet, etc).\ncase class Person(firstName: String, lastName:String) // wire-in spark implicits import spark.implicits._ case class Person(firstName: String, lastName: String) val ds = Seq(Person(\u0026#34;Daniel\u0026#34;, \u0026#34;Williams\u0026#34;)).toDS() // here you can perform operations that are deferred until an action is invoked. // creates a anonymous lambda that looks at the // firstName of the Dataset[Person] type and invokes a collect // to pull data back to the driver as an Array[Person] // the foreach then will invoke a println on each Person // instance and implicit apply the toString operation that is // held in the Product trait ds.filter(_.firstName == \u0026#34;Daniel\u0026#34;).collect().foreach(println)  Spark SQL, DataFrames and Datasets Guide  Further Reading For the complete list of RDD operations, please see the Spark Programming Guide.\n"});index.add({'id':13,'href':'/docs/sessions/spark-graphx/','title':"Spark Graphx",'content':"Spark GraphX Learning Objectives\n Understand composition of a graph in Spark GraphX. Being able to create a graph. Being able to use the built-in graph algorithm.   In this section we begin by creating a graph with patient and diagnostic codes. Later we will show how to run graph algorithms on the the graph you will create.\nBasic concept Spark GraphX abstracts a graph as a concept named Property Graph, which means that each edge and vertex is associated with some properties. The Graph class has the following definition\nclass Graph[VD, ED] { val vertices: VertexRDD[VD] val edges: EdgeRDD[ED] } Where VD and ED define property types of each vertex and edge respectively. We can regard VertexRDD[VD] as RDD of (VertexID, VD) tuple and EdgeRDD[ED] as RDD of (VertexID, VertexID, ED).\nGraph construction Let\u0026rsquo;s create a graph of patients and diagnostic codes. For each patient we can assign its patient id as vertex property, and for each diagnostic code, we will use the code as vertex property. For the edge between patient and diagnostic code, we will use number of times the patient is diagnosed with given disease as edge property.\nDefine class Let\u0026rsquo;s first define necessary data structure and import\nimport org.apache.spark.SparkContext._ import org.apache.spark.graphx._ import org.apache.spark.rdd.RDD abstract class VertexProperty extends Serializable case class PatientProperty(patientId: String) extends VertexProperty case class DiagnosticProperty(icd9code: String) extends VertexProperty case class PatientEvent(patientId: String, eventName: String, date: Int, value: Double) Load raw data Load patient event data and filter out diagnostic related events only\nval allEvents = sc.textFile(\u0026#34;data/\u0026#34;). map(_.split(\u0026#34;,\u0026#34;)). map(splits =\u0026gt; PatientEvent(splits(0), splits(1), splits(2).toInt, splits(3).toDouble)) // get and cache diagnosticEvents as we will reuse val diagnosticEvents = allEvents. filter(_.eventName.startsWith(\u0026#34;DIAG\u0026#34;)).cache() Create vertex Patient vertex Let\u0026rsquo;s create patient vertex\n// create patient vertex val patientVertexIdRDD = diagnosticEvents. map(_.patientId). distinct. // get distinct patient ids  zipWithIndex // assign an index as vertex id  val patient2VertexId = patientVertexIdRDD.collect.toMap val patientVertex = patientVertexIdRDD. map{case(patientId, index) =\u0026gt; (index, PatientProperty(patientId))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] In order to use the newly created vertex id, we finally collect all the patient to VertrexID mapping.\nTheoretically, collecting RDD to driver is not an efficient practice. One can obtain uniqueness of ID by calculating ID directly with a Hash.  Diagnostic code vertex Similar to patient vertex, we can create diagnostic code vertex with\n// create diagnostic code vertex val startIndex = patient2VertexId.size val diagnosticVertexIdRDD = diagnosticEvents. map(_.eventName). distinct. zipWithIndex. map{case(icd9code, zeroBasedIndex) =\u0026gt; (icd9code, zeroBasedIndex + startIndex)} // make sure no conflict with patient vertex id  val diagnostic2VertexId = diagnosticVertexIdRDD.collect.toMap val diagnosticVertex = diagnosticVertexIdRDD. map{case(icd9code, index) =\u0026gt; (index, DiagnosticProperty(icd9code))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] Here we assign vertex id by adding the result of zipWithIndex with an offset obtained from previous patient vertex to avoid ID confliction between patient and diagnostic code.\nCreate edge In order to create edge, we will need to know vertext id of vertices we just created.\nval bcPatient2VertexId = sc.broadcast(patient2VertexId) val bcDiagnostic2VertexId = sc.broadcast(diagnostic2VertexId) val edges = diagnosticEvents. map(event =\u0026gt; ((event.patientId, event.eventName), 1)). reduceByKey(_ + _). map{case((patientId, icd9code), count) =\u0026gt; (patientId, icd9code, count)}. map{case(patientId, icd9code, count) =\u0026gt; Edge( bcPatient2VertexId.value(patientId), // src id  bcDiagnostic2VertexId.value(icd9code), // target id  count // edge property  )} We first broadcast patient and diagnostic code to vertext id mapping. Broadcast can avoid unnecessary copy in distributed setting thus will be more effecient. Then we count occurrence of (patient-id, icd-9-code) pairs with map and reduceByKey, finally we translate them to proper VertexID.\nAssemble vertex and edge We will need to put vertices and edges together to create the graph\nval vertices = sc.union(patientVertex, diagnosticVertex) val graph = Graph(vertices, edges) Graph operation Given the graph we created, we can run some basic graph operations.\nConnected components Connected component can help find disconnected subgraphs. GraphX provides the API to get connected components as below\nval connectedComponents = graph.connectedComponents The return result is a graph and assigned components of original graph is stored as VertexProperty. For example\nscala\u0026gt; connectedComponents.vertices.take(5) Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((2556,0), (1260,0), (1410,0), (324,0), (180,0)) The first element of the tuple is VertexID identical to original graph. The second element in the tuple is connected component represented by the lowest-numbered VertexID in that component. In above example, five vertices belong to same component.\nWe can easily get number of connected components using operations on RDD as below.\nscala\u0026gt; connectedComponents.vertices.map(_._2).distinct.collect Array[org.apache.spark.graphx.VertexId] = Array(0, 169, 239) Degree The property graph abstraction of GraphX is a directed graph. It provides computation of in-dgree, out-degree and total degree. For example, we can get degrees as\nval inDegrees = graph.inDegrees val outDegrees = graph.outDegrees val totalDegrees = graph.degrees PageRank GraphX also provides implementation of the famous PageRank algorithm, which can compute the \u0026lsquo;importance\u0026rsquo; of a vertex. The graph we generated above is a bipartite graph and not suitable for PageRank. To gve an example of PageRank, we randomly generate a graph and run fixed iteration of PageRank algorithm on it.\nimport org.apache.spark.graphx.util.GraphGenerators val randomGraph:Graph[Long, Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100) val pagerank = randomGraph.staticPageRank(20) Or, we can run PageRank until converge with tolerance as 0.01 using randomGraph.pageRank(0.01)\nApplication Next, we show some how we can ultilize the graph operations to solve some practical problems in the healthcare domain.\nExplore comorbidities Comorbidity is additional disorders co-occuring with primary disease. We know all the case patients have heart failure, we can explore possible comorbidities as below (see comments for more explaination)\n// get all the case patients val casePatients = allEvents. filter(event =\u0026gt; event.eventName == \u0026#34;heartfailure\u0026#34; \u0026amp;\u0026amp; event.value == 1.0). map(_.patientId). collect. toSet // broadcast val scCasePatients = sc.broadcast(casePatients) //filter the graph with subGraph operation val filteredGraph = graph.subgraph(vpred = {case(id, attr) =\u0026gt; val isPatient = attr.isInstanceOf[PatientProperty] val patient = if(isPatient) attr.asInstanceOf[PatientProperty] else null // return true iff. isn\u0026#39;t patient or is case patient  !isPatient || (scCasePatients.value contains patient.patientId) }) //calculate indegrees and get top vertices val top5ComorbidityVertices = filteredGraph.inDegrees. takeOrdered(5)(scala.Ordering.by(-_._2)) We have\ntop5ComorbidityVertices: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3129,86), (335,63), (857,58), (2048,49), (669,48)) And we can check the vertex of index 3129 in original graph is\nscala\u0026gt; graph.vertices.filter(_._1 == 3129).collect Array[(org.apache.spark.graphx.VertexId, VertexProperty)] = Array((3129,DiagnosticProperty(DIAG4019))) The 4019 code correponds to Hypertension, which is reasonable.\nSimilar patients Given a patient diagnostic graph, we can also find similar patients. One of the most straightforward approach is shortest path on the graph.\nval sssp = graph. mapVertices((id, _) =\u0026gt; if (id == 0L) 0.0 else Double.PositiveInfinity). pregel(Double.PositiveInfinity)( (id, dist, newDist) =\u0026gt; math.min(dist, newDist), // Vertex Program  triplet =\u0026gt; { // Send Message  var msg: Iterator[(org.apache.spark.graphx.VertexId, Double)] = Iterator.empty if (triplet.srcAttr + 1 \u0026lt; triplet.dstAttr) { msg = msg ++ Iterator((triplet.dstId, triplet.srcAttr + 1)) } if (triplet.dstAttr + 1 \u0026lt; triplet.srcAttr) { msg = msg ++ Iterator((triplet.srcId, triplet.dstAttr + 1)) } println(msg) msg }, (a,b) =\u0026gt; math.min(a,b) // Merge Message  ) // get top 5 most similar sssp.vertices.filter(_._2 \u0026lt; Double.PositiveInfinity). filter(_._1 \u0026lt; 300). takeOrdered(5)(scala.Ordering.by(-_._2)) "});index.add({'id':14,'href':'/docs/sessions/spark-mllib/','title':"Spark Mllib",'content':"Spark MLlib and Scikit-learn Learning Objectives\n Understand input to MLlib. Learn to run basic classification algorithms. Learn to export/load trained models. Develop models using python machine learning module.   In this section, you will learn how to build a heart failure (HF) predictive model. You should have finished previous Spark Application section. You will first learn how to train a model using Spark MLlib and save it. Next, you will learn how to achieve same goal using Python Scikit-learn machine learning module for verification purpose.\nMLlib You will first load data and compute some high-level summary statistics, then train a classifier to predict heart failure.\nLoad Samples Loading data from previously saved data can be achieved by\nimport org.apache.spark.mllib.util.MLUtils val data = MLUtils.loadLibSVMFile(sc, \u0026#34;samples\u0026#34;) Basic Statistics Spark MLlib provides various functions to compute summary statistics that are useful when doing machine learning and data analysis tasks.\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics} // colStats() calculates the column statistics for RDD[Vector] // we need to extract only the features part of each LabeledPoint: // RDD[LabeledPoint] =\u0026gt; RDD[Vector] val summary = Statistics.colStats(data.map(_.features)) // summary.mean: a dense vector containing the mean value for each feature (column) // the mean of the first feature is 0.3 summary.mean(0) // the variance of the first feature summary.variance(0) // the number of non-zero values of the first feature summary.numNonzeros(0) Split data In a typical machine learning problem, we need to split data into training (60%) and testing (40%) set.\nval splits = data.randomSplit(Array(0.6, 0.4), seed = 15L) val train = splits(0).cache() val test = splits(1).cache() Train classifier Let\u0026rsquo;s train a linear SVM model using Stochastic Gradient Descent (SGD) on the training set to predict heart failure\nimport org.apache.spark.mllib.classification.SVMWithSGD val numIterations = 100 val model = SVMWithSGD.train(train, numIterations) Testing For each sample in the testing set, output a (prediction, label) pair, and calculate the prediction accuracy. We use the broadcast mechanism to avoid unnecessary data copy.\nval scModel = sc.broadcast(model) val predictionAndLabel = test.map(x =\u0026gt; (scModel.value.predict(x.features), x.label)) val accuracy = predictionAndLabel.filter(x =\u0026gt; x._1 == x._2).count / test.count.toFloat println(\u0026#34;testing Accuracy = \u0026#34; + accuracy) Save \u0026amp; load model In real world setting, you may need to save the trained model. You can achieve that by directly serialize you model object using java ObjectOutputStream and save\nimport java.io.{FileOutputStream, ObjectOutputStream, ObjectInputStream, FileInputStream} // save model  val oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;model\u0026#34;)) oos.writeObject(model) oos.close() // load model from disk  val ois = new ObjectInputStream(new FileInputStream(\u0026#34;model\u0026#34;)) val loadedModel = ois.readObject().asInstanceOf[org.apache.spark.mllib.classification.SVMModel] ois.close() Scikit-learn If typical data set is often small enough after feature construction described in previous Spark Application section, you may consider running machine learning predictive model training and testing using your familiar tools like scikit-learn in Python or some R packages. Here we show how to do that in Scikit-learn, a Python machine learning library.\nFetch data In order to work with Scikit-learn, you will need to take data out of HDFS into a local file system. We can get the samples folder from your home directory in HDFS and merge content into one single file with the command below\nhdfs dfs -getmerge samples patients.svmlight Move on with Python In later steps, you will use python interactive shell. To open a python interactive shell, just type python in bash. You will get prompt similar to the sample below\n[hang@bootcamp1 ~]$ python Python 2.7.10 |Continuum Analytics, Inc.| (default, Oct 19 2015, 18:04:42) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. Anaconda is brought to you by Continuum Analytics. Please check out: http://continuum.io/thanks and https://anaconda.org \u0026gt;\u0026gt;\u0026gt; which show version and distribution of the python installation you are using. Here we pre-installed Anaconda\nLoad and split data Now we can load data and split it into training and testing set in similar way as the MLlib approach.\nfrom sklearn.cross_validation import train_test_split from sklearn.datasets import load_svmlight_file X, y = load_svmlight_file(\u0026#34;patients.svmlight\u0026#34;) X = X.toarray() # make it dense X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=41) Train classifier Let\u0026rsquo;s train a linear SVM model again on the training set to predict heart failure\nfrom sklearn.svm import LinearSVC model = LinearSVC(C=1.0, random_state=42) model.fit(X_train, y_train) Testing We can get prediction accuracy and AUC on testing set as\nfrom sklearn.metrics import roc_auc_score accuracy = model.score(X_test, y_test) y_score = model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print \u0026#34;accuracy = %.3f, AUC = %.3f\u0026#34; % (accuracy, auc) Save \u0026amp; load model We can save and load the trained model via pickle serialization module in Python like\nimport pickle with open(\u0026#39;pysvcmodel.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(model, f) with open(\u0026#39;pysvcmodel.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f: loaded_model = pickle.load(f) Sparsity and predictive features Since we have limited training data but a large number of features, we may consider using L1 penalty on model to regularize parameters.\nfrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) l1_model = LinearSVC(C=1.0, random_state=42, dual=False, penalty=\u0026#39;l1\u0026#39;) l1_model.fit(X_train, y_train) accuracy = l1_model.score(X_test, y_test) y_score = l1_model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print \u0026#34;for sparse model, accuracy = %.3f, auc = %.3f\u0026#34; % (accuracy, auc) Before fitting a model, we scaled the data to make sure weights of features are comparable. With the sparse model we get from previous example, we can actually identify predictive features according to their coefficients. Here we assume you did the last exercise of previous section about Spark Application. If not, please do that first.\nimport numpy as np ## loading mapping mapping = [] with open(\u0026#39;mapping.txt\u0026#39;) as f: for line in f.readlines(): splits = line.split(\u0026#39;|\u0026#39;) # feature-name | feature-index mapping.append(splits[0]) ## get last 10 - the largest 10 indices top_10 =np.argsort(l1_model.coef_[0])[-10:] for index, fid in enumerate(top_10[::-1]): #read in reverse order print \u0026#34;%d: feature [%s] with coef %.3f\u0026#34; % (index, mapping[fid], l1_model.coef_[0][fid]) Regression Suppose now instead of predicting whether a patient has heart failure, we want to predict the total amount of payment for each patient. This is no longer a binary classification problem, because the labels we try to predict are real-valued numbers. In this case, we can use the regression methods in MLlib.\nConstruct data We need to construct a new dataset for this regression problem. The only difference is that we change the label from heartfailure (binary) to PAYMENT (real value).\nscala\u0026gt; val labelID = featureMap(\u0026#34;PAYMENT\u0026#34;) scala\u0026gt; val data = features.map{ case ((patient, feature), value) =\u0026gt; (patient, (feature, value)) }. groupByKey(). map{ x =\u0026gt; val label = x._2.find(_._1 == labelID).get._2 val featureNoLabel = x._2.toSeq.filter(_._1 != labelID) LabeledPoint(label, Vectors.sparse(numOfFeatures, featureNoLabel)) } Split data Split data into training (60%) and test (40%) set.  scala\u0026gt; val splits = data.randomSplit(Array(0.6, 0.4), seed = 0L) scala\u0026gt; val train = splits(0).cache() scala\u0026gt; val test = splits(1).cache() Training Train a linear regression model using SGD on the training set\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD scala\u0026gt; val numIterations = 100 scala\u0026gt; val model = LinearRegressionWithSGD.train(training, numIterations) Testing For each example in the testing set, output a (prediction, label) pair, and calculate the mean squared error.\nscala\u0026gt; val predictionAndLabel = test.map(x =\u0026gt; (model.predict(x.features), x.label)) scala\u0026gt; val MSE = predictionAndLabel.map{case(p, l) =\u0026gt; math.pow((p - l), 2)}.mean() scala\u0026gt; println(\u0026#34;testing Mean Squared Error = \u0026#34; + MSE) "});index.add({'id':15,'href':'/docs/sessions/spark-sql/','title':"Spark Sql",'content':"Spark Sql Learning Objectives\n Load data into Spark SQL as DataFrame. Manipulate data with built-in functions. Define a User Defined Function (UDF).   Overview Recent versions of Spark released the programming abstraction named DataFrame, which can be regarded as a table in a relational database. DataFrame is stored in a distributed manner so that different rows may locate on different machines. On DataFrame you can write sql queries, manipulate columns programatically with API etc.\nLoading data Spark provides an API to load data from JSON, Parquet, Hive table etc. You can refer to the official Spark SQL programming guide for those formats. Here we show how to load csv files. And we will use the spark-csv module by Databricks.\nStart the Spark shell in local mode with the command below to add extra dependencies which are needed to complete this training.\n% spark-shell --master \u0026#34;local[2]\u0026#34; --driver-memory 3G --packages com.databricks:spark-csv_2.11:1.5.0 [logs] Spark context available as sc. 15/05/04 13:12:57 INFO SparkILoop: Created sql context (with Hive support).. SQL context available as sqlContext. scala\u0026gt; Spark 2.0+ has built-in csv library now. This parameter is not required any more, and it is only used as a sample.  You may want to hide the log messages from spark. You can achieve that by\nimport org.apache.log4j.Logger import org.apache.log4j.Level Logger.getRootLogger.setLevel(Level.ERROR)   Now load data into the shell.\nscala\u0026gt; val sqlContext = spark.sqlContext sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5cef5fc9 scala\u0026gt; val patientEvents = sqlContext.load(\u0026#34;input/\u0026#34;, \u0026#34;com.databricks.spark.csv\u0026#34;). toDF(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;rawvalue\u0026#34;). withColumn(\u0026#34;value\u0026#34;, \u0026#39;rawvalue.cast(\u0026#34;Double\u0026#34;)) patientEvents: org.apache.spark.sql.DataFrame = [patientId: string, eventId: string, date: string, rawvalue: string, value: double] The first parameter is path to the data (in HDFS), and second is a class name, the specific adapter required to load a CSV file. Here we specified a directory name instead of a specific file name so that all files in that directory will be read and combined into one file. Next we call toDF to rename the columns in the CSV file with meaningful names. Finally, we add one more column that has double type of value instead of string which we will use ourselves for the rest of this material.\nManipulating data There are two methods to work with the DataFrame, either using SQL or using domain specific language (DSL).\nSQL Writing SQL is straightforward assuming you have experiences with relational databases.\nscala\u0026gt; patientEvents.registerTempTable(\u0026#34;events\u0026#34;) scala\u0026gt; sqlContext.sql(\u0026#34;select patientId, eventId, count(*) count from events where eventId like \u0026#39;DIAG%\u0026#39; group by patientId, eventId order by count desc\u0026#34;).collect res5: Array[org.apache.spark.sql.Row] = Array(...) Here the patientEvents DataFrame is registered as a table in sql context so that we could run sql commands. Next line is a standard sql command with where, group by and order by statements.\nDSL Next, we show how to manipulate data with DSL, the same result of previous SQL command can be achieved by:\nscala\u0026gt; patientEvents.filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)).groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;).count.orderBy($\u0026#34;count\u0026#34;.desc).show patientId eventId count 00291F39917544B1 DIAG28521 16 00291F39917544B1 DIAG58881 16 00291F39917544B1 DIAG2809 13 00824B6D595BAFB8 DIAG4019 11 0085B4F55FFA358D DIAG28521 9 6A8F2B98C1F6F5DA DIAG58881 8 019E4729585EF3DD DIAG4019 8 0124E58C3460D3F8 DIAG4019 8 2D5D3D5F03C8C176 DIAG4019 8 01A999551906C787 DIAG4019 7 ... For complete DSL functions, see DataFrame class API.\nSaving data Spark SQL provides a convenient way to save data in a different format just like loading data. For example you can write\nscala\u0026gt; patientEvents. filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)). groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;). count. orderBy($\u0026#34;count\u0026#34;.desc). write.json(\u0026#34;aggregated.json\u0026#34;) to save your transformed data in json format or\nscala\u0026gt; patientEvents. filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)). groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;).count. orderBy($\u0026#34;count\u0026#34;.desc). write.format(\u0026#34;com.databricks.spark.csv\u0026#34;).save(\u0026#34;aggregated.csv\u0026#34;) to save in csv format.\nUDF In many cases the built-in function of SQL like count, max is not enough, you can extend it with your own functions. For example, you want to find the number of different event types with the following UDF.\nDefine Define and register an UDF\nscala\u0026gt; sqlContext.udf.register(\u0026#34;getEventType\u0026#34;, (s: String) =\u0026gt; s match { case diagnostics if diagnostics.startsWith(\u0026#34;DIAG\u0026#34;) =\u0026gt; \u0026#34;diagnostics\u0026#34; case \u0026#34;PAYMENT\u0026#34; =\u0026gt; \u0026#34;payment\u0026#34; case drug if drug.startsWith(\u0026#34;DRUG\u0026#34;) =\u0026gt; \u0026#34;drug\u0026#34; case procedure if procedure.startsWith(\u0026#34;PROC\u0026#34;) =\u0026gt; \u0026#34;procedure\u0026#34; case \u0026#34;heartfailure\u0026#34; =\u0026gt; \u0026#34;heart failure\u0026#34; case _ =\u0026gt; \u0026#34;unknown\u0026#34; }) Use Write sql and call your UDF\nscala\u0026gt; sqlContext.sql(\u0026#34;select getEventType(eventId) type, count(*) count from events group by getEventType(eventId) order by count desc\u0026#34;).show type count drug 16251 diagnostics 10820 payment 3259 procedure 514 heart failure 300  SQL  scala\u0026gt; sqlContext.sql(\u0026#34;select patientId, sum(value) as payment from events where eventId = \u0026#39;PAYMENT\u0026#39; group by patientId order by payment desc limit 10\u0026#34;).show patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0  DSL  scala\u0026gt; patientEvents.filter(patientEvents(\u0026#34;eventId\u0026#34;) === \u0026#34;PAYMENT\u0026#34;).groupBy(\u0026#34;patientId\u0026#34;).agg(\u0026#34;value\u0026#34; -\u0026gt; \u0026#34;sum\u0026#34;).withColumnRenamed(\u0026#34;sum(value)\u0026#34;, \u0026#34;payment\u0026#34;).orderBy($\u0026#34;payment\u0026#34;.desc).show(10) patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0 "});index.add({'id':16,'href':'/docs/sessions/spark/','title':"Spark",'content':"Overview of Spark Learning Objectives\n Learn basics about Scala programming language. Understand Spark RDD operations. Acquire hands-on experiences using Spark for analytics.   In this chapter, you will learn about Spark, an in-memory big data computing framework for parallel data processing and analytics. In this training we will illustrate several components of Spark ecosystem through the interactive shell.\nSpark is mainly developed with Scala, a functional programming language on JVM. Though most of the Spark functions also have Python and Java API, we will only present examples in Scala for its conciseness and simplicity. Interested students can learn more about Python and Java in Spark from the official Spark document.\nThis chapter is divided into following sections:\n Scala Basics: You will learn basic Scala syntax via interactive shell, including declaring variables of different types, making function calls as well as how to compile and run a standalone Scala program. Spark Basics: You will learn how to load data into Spark and how to conduct some basic processing, e.g., converting data from raw string into a predefined class, filtering out those items with missing fields and statistics calculation. Spark SQL: You will learn how to use SQL like syntax for data processing in Spark. You will see how the data processing tasks can be achieved with Spark SQL. Spark Application: You will learn how to preprocess data using spark for predictive modeling. Specifically, you will setup patient features and target for later heart failure prediction using MLlib and using Scikit-learn (Python machine learning module). Spark MLlib and Scikit-learn: With the pre-processed data from the previous section, you will have a dataset suitable for Machine Learning tasks. In this section, you will learn how to apply existing algorithms in MLlib and in Scikit-learn to predict whether a patient will have heart failure. Spark GraphX: GraphX is a Spark component for graph data processing. In this section, you will learn how to construct a graph and run graph algorithms such as PageRank, connected components on large graph.  "});index.add({'id':17,'href':'/docs/sessions/zeppelin-intro/','title':"Zeppelin Intro",'content':"How to start Zeppelin ::: tip Learning Objectives\n Learn how to work with Zeppelin Notebook.  :::\n::: warning\nYou can skip this section, if you use your locally installed Zeppelin\n:::\n1. Run provided Docker image Please prepare your docker environment and refer to this section to start your zeppelin service.\nShared Folder You can use shared folder between your local OS and the virtual environment on Docker. This shared folder can be used to get data from your local and/or to save data without losing it after you exit/destroy your virtual environment. Use -v option to make shared folder from an existing local folder and a folder in virtual environment:\n -v \u0026lt;local_folder:vm_folder\u0026gt;\n You should use absolute path for vm_folder, but it does not need to be an existing folder. For example, if want to use ~/Data/ in my local OS as shared folder connected with /sample_data/ in VM, I can start a container as following:\ndocker run -it --privileged=true \\  --cap-add=SYS_ADMIN \\  -m 8192m -h bootcamp1.docker \\  --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\  -v /path/to/Data/:/sample_data/ \\  sunlab/bigbox:latest \\  /bin/bash 2. Start Zeppelin service and create HDFS folder If you have not started Zeppelin service,\n/scripts/start-zeppelin.sh We need to create a HDFS folder for the user zeppelin as:\nsudo su - hdfs # switch to user \u0026#39;hdfs\u0026#39; hdfs dfs -mkdir -p /user/zeppelin # create folder in hdfs hdfs dfs -chown zeppelin /user/zeppelin # change the folder owner exit You can check whether it has been created or not by using:\nhdfs dfs -ls /user/ 3. Open Zeppelin Notebook in your browser Once you have started Zeppelin service and have created HDFS folder for Zeppelin, you can access Zeppelin Notebook by using your local web browser.\nOpen your web browser, and type in the address: host-ip:port-for-zeppelin For example, 192.168.99.100:9530 since the IP address assigned to my Docker container is 192.168.99.100 as it is shown above, and the port number assigned to Zeppelin service is 9530 as default in our Docker image.\nOnce you navigate to that IP address with the port number, you will see the front page of Zeppelin like: Let\u0026rsquo;s move to do a simple tutorial in the next section.\n"});index.add({'id':18,'href':'/docs/sessions/zeppelin-tutorial/','title':"Zeppelin Tutorial",'content':"Zeppelin Basic Tutorial ::: tip Learning Objectives\n Try to follow the official tutorial of Zeppelin Notebook step-by-step.  :::\n1. Create a new Notebook Click on \u0026lsquo;Create new note\u0026rsquo;, and give a name, click on \u0026lsquo;Create Note\u0026rsquo;: Then, you will see a new blank note:\nNext, click the gear icon on the top-right, interpreter binding setting will be unfolded. Default interpreters will be enough for the most of cases, but you can add/remove at \u0026lsquo;interpreter\u0026rsquo; menu if you want to. Click on \u0026lsquo;Save\u0026rsquo; once you complete your configuration.\n2. Basic usage You can click the gear icon at the right side of the paragraph. If you click \u0026lsquo;Show title\u0026rsquo; you can give a title as you want for each paragraph. Try to use other commands also.\nText note Like other Notebooks, e.g. Jupyter, we can put some text in a paragraph by using md command with Markdown syntax:\n%md \u0026lt;some text using markdown syntax\u0026gt; After put text, click play button or Shift+Enter to run the paragraph. It will show formatted Markdown text. You can also choose show/hide editor for better visual or publishing.\nScala code If you bind default interpreters, you can use scala codes as well as Spark API directly in a paragraph:\nAgain, do not forget to click play button or Shift+Enter to actually run the paragraph.\nPossible Error If you meet an error related with HDFS, please check whether you have created HDFS user folder for \u0026lsquo;zeppelin\u0026rsquo; as described in Zeppelin-Intro\n3. Load Data Into Table We can use sql query statements for easier visualization with Zeppelin. Later, you can fully utilize Angular or D3 in Zeppelin for better or more sophisticated visualization.\nLet\u0026rsquo;s get \u0026lsquo;Bank\u0026rsquo; data from the official Zeppelin tutorial.\nNext, define a case class for easy transformation into DataFrame and map the text data we downloaded into DataFrame without its header. Finally, register this DataFrame as Table to use sql query statements.\n4. Visualization of Data via SQL query statement Once data is loaded into Table, you can use SQL query to visualize data you want to see:\n%sql \u0026lt;valid SQL statement\u0026gt; Let\u0026rsquo;s try to show a distribution of age of who are younger than 30.\nAs you can see, visualization tool will be automatically loaded once you run a paragraph with SQL statement. Default one is the result table of the query statement, but you can choose other types of visualization such as bar chart, pie chart and line chart by just clicking the icons.\nAlso, you can change configurations for each chart as you want\nInput Form You can create input form by using ${formName} or ${formName=defaultValue} templates.\nSelect Form Also, you can create select form by using ${formName=defaultValue,option1|option2...}\nFor more dynamic forms, please refer to zeppelin-dynamicform\n5. Export/Import Notebook Once you finish your works, you can export Notebook as JSON file for later use.\nAlso, you can import Notebook exported as JSON or from URL.\nTutorial File You can download the JSON file for this tutorial here or see the official \u0026lsquo;Zeppelin Tutorial\u0026rsquo; on the frontpage of Zeppelin.\n"});index.add({'id':19,'href':'/docs/','title':"Docs",'content':""});})();