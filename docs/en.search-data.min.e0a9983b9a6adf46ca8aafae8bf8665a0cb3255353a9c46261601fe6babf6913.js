'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/example/','title':"Example Site",'content':"Introduction Ferre hinnitibus erat accipitrem dixi Troiae tollens Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\n Pedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret  Est simul fameque tauri qua ad Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue, viralItunesBalancing, bankruptcy_file_pptp)) { file += ip_cybercrime_suffix; } if (runtimeSmartRom == netMarketingWord) { virusBalancingWin *= scriptPromptBespoke + raster(post_drive, windowsSli); cd = address_hertz_trojan; soap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui); } else { megabyte.api = modem_flowchart - web + syntaxHalftoneAddress; } if (3 \u0026lt; mebibyteNetworkAnimated) { pharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle( dvrSyntax, cdma); adf_sla *= hoverCropDrive; templateNtfs = -1 - vertical; } else { expressionCompressionVariable.bootMulti = white_eup_javascript( table_suffix); guidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1, managementRosetta(webcamActivex), 740874); } var virusTweetSsl = nullGigo;  Trepident sitimque Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"});index.add({'id':1,'href':'/docs/example/table-of-contents/with-toc/','title':"With ToC",'content':"Caput vino delphine in tamen vias Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  Natus quaerere Pectora et sine mulcere, coniuge dum tincta incurvae. Quis iam; est dextra Peneosque, metuis a verba, primo. Illa sed colloque suis: magno: gramen, aera excutiunt concipit.\n Phrygiae petendo suisque extimuit, super, pars quod audet! Turba negarem. Fuerat attonitus; et dextra retinet sidera ulnas undas instimulat vacuae generis? Agnus dabat et ignotis dextera, sic tibi pacis feriente at mora euhoeque comites hostem vestras Phineus. Vultuque sanguine dominoque metuit risi fama vergit summaque meus clarissimus artesque tinguebat successor nominis cervice caelicolae.\n Limitibus misere sit Aurea non fata repertis praerupit feruntur simul, meae hosti lentaque citius levibus, cum sede dixit, Phaethon texta. Albentibus summos multifidasque iungitur loquendi an pectore, mihi ursaque omnia adfata, aeno parvumque in animi perlucentes. Epytus agis ait vixque clamat ornum adversam spondet, quid sceptra ipsum est. Reseret nec; saeva suo passu debentia linguam terga et aures et cervix de ubera. Coercet gelidumque manus, doluit volvitur induta?\nEnim sua Iuvenilior filia inlustre templa quidem herbis permittat trahens huic. In cruribus proceres sole crescitque fata, quos quos; merui maris se non tamen in, mea.\nGermana aves pignus tecta Mortalia rudibusque caelum cognosceret tantum aquis redito felicior texit, nec, aris parvo acre. Me parum contulerant multi tenentem, gratissime suis; vultum tu occupat deficeret corpora, sonum. E Actaea inplevit Phinea concepit nomenque potest sanguine captam nulla et, in duxisses campis non; mercede. Dicere cur Leucothoen obitum?\nPostibus mittam est nubibus principium pluma, exsecratur facta et. Iunge Mnemonidas pallamque pars; vere restitit alis flumina quae quoque, est ignara infestus Pyrrha. Di ducis terris maculatum At sede praemia manes nullaque!\n"});index.add({'id':2,'href':'/docs/example/table-of-contents/without-toc/','title':"Without ToC",'content':"At me ipso nepotibus nunc celebratior genus Tanto oblite Lorem markdownum pectora novis patenti igne sua opus aurae feras materiaque illic demersit imago et aristas questaque posset. Vomit quoque suo inhaesuro clara. Esse cumque, per referri triste. Ut exponit solisque communis in tendens vincetis agisque iamque huic bene ante vetat omina Thebae rates. Aeacus servat admonitu concidit, ad resimas vultus et rugas vultu dignamque Siphnon.\nQuam iugulum regia simulacra, plus meruit humo pecorumque haesit, ab discedunt dixit: ritu pharetramque. Exul Laurenti orantem modo, per densum missisque labor manibus non colla unum, obiectat. Tu pervia collo, fessus quae Cretenque Myconon crate! Tegumenque quae invisi sudore per vocari quaque plus ventis fluidos. Nodo perque, fugisse pectora sorores.\nSumme promissa supple vadit lenius Quibus largis latebris aethera versato est, ait sentiat faciemque. Aequata alis nec Caeneus exululat inclite corpus est, ire tibi ostendens et tibi. Rigent et vires dique possent lumina; eadem dixit poma funeribus paret et felix reddebant ventis utile lignum.\n Remansit notam Stygia feroxque Et dabit materna Vipereas Phrygiaeque umbram sollicito cruore conlucere suus Quarum Elis corniger Nec ieiunia dixit  Vertitur mos ortu ramosam contudit dumque; placabat ac lumen. Coniunx Amoris spatium poenamque cavernis Thebae Pleiadasque ponunt, rapiare cum quae parum nimium rima.\nQuidem resupinus inducto solebat una facinus quae Credulitas iniqua praepetibus paruit prospexit, voce poena, sub rupit sinuatur, quin suum ventorumque arcadiae priori. Soporiferam erat formamque, fecit, invergens, nymphae mutat fessas ait finge.\n Baculum mandataque ne addere capiti violentior Altera duas quam hoc ille tenues inquit Sicula sidereus latrantis domoque ratae polluit comites Possit oro clausura namque se nunc iuvenisque Faciem posuit Quodque cum ponunt novercae nata vestrae aratra  Ite extrema Phrygiis, patre dentibus, tonso perculit, enim blanda, manibus fide quos caput armis, posse! Nocendo fas Alcyonae lacertis structa ferarum manus fulmen dubius, saxa caelum effuge extremis fixum tumor adfecit bella, potentes? Dum nec insidiosa tempora tegit spirarunt. Per lupi pars foliis, porreximus humum negant sunt subposuere Sidone steterant auro. Memoraverit sine: ferrum idem Orion caelum heres gerebat fixis?\n"});index.add({'id':3,'href':'/docs/example/table-of-contents/','title':"Table of Contents",'content':"Ubi loqui Mentem genus facietque salire tempus bracchia Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"});index.add({'id':4,'href':'/docs/example/collapsed/','title':"Collapsed",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':5,'href':'/docs/example/collapsed/3rd-level/4th-level/','title':"4th Level",'content':"4th Level of Menu Caesorum illa tu sentit micat vestes papyriferi Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"});index.add({'id':6,'href':'/docs/example/collapsed/3rd-level/','title':"3rd Level",'content':"3rd Level of Menu Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  "});index.add({'id':7,'href':'/docs/example/hidden/','title':"Hidden",'content':"This page is hidden in menu Quondam non pater est dignior ille Eurotas Latent te facies Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\n Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor  Cum honorum Latona O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer), pad.property_data_programming.sectorBrowserPpga(dataMask, 37, recycleRup)); intellectualVaporwareUser += -5 * 4; traceroute_key_upnp /= lag_optical(android.smb(thyristorTftp)); surge_host_golden = mca_compact_device(dual_dpi_opengl, 33, commerce_add_ppc); if (lun_ipv) { verticalExtranet(1, thumbnail_ttl, 3); bar_graphics_jpeg(chipset - sector_xmp_beta); }  Fronde cetera dextrae sequens pennis voce muneris Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software; if (internic \u0026gt; disk) { emoticonLockCron += 37 + bps - 4; wan_ansi_honeypot.cardGigaflops = artificialStorageCgi; simplex -= downloadAccess; } var volumeHardeningAndroid = pixel + tftp + onProcessorUnmount; sector(memory(firewire + interlaced, wired)); "});index.add({'id':8,'href':'/docs/shortcodes/','title':"Shortcodes",'content':""});index.add({'id':9,'href':'/docs/shortcodes/buttons/','title':"Buttons",'content':"Buttons Buttons are styled links that can lead to local page or external link.\nExample {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}}  Get Home  Contribute  "});index.add({'id':10,'href':'/docs/shortcodes/columns/','title':"Columns",'content':"Columns Columns help organize shorter pieces of content horizontally for readability.\n{{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic sparator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic sparator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} Example Left Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.  Mid Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter!  Right Content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.   "});index.add({'id':11,'href':'/docs/shortcodes/expand/','title':"Expand",'content':"Expand Expand shortcode can help to decrease clutter on screen by hiding part of text. Expand content by clicking on it.\nExample Default {{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  Expand ↕  Markdown content Lorem markdownum insigne\u0026hellip;    With Custom Label {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}}  Custom Label ...  Markdown content Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.    "});index.add({'id':12,'href':'/docs/shortcodes/hints/','title':"Hints",'content':"Hints Hint shortcode can be used as hint/alerts/notification block.\nThere are 3 colors to choose: info, warning and danger.\n{{\u0026lt; hint [info|warning|danger] \u0026gt;}} **Markdown content** Lorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa {{\u0026lt; /hint \u0026gt;}} Example Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  Markdown content\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa  "});index.add({'id':13,'href':'/docs/shortcodes/katex/','title':"Katex",'content':"KaTeX KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nExample {{\u0026lt; katex [display] [class=\u0026#34;text-center\u0026#34;] \u0026gt;}} x = \\begin{cases} a \u0026amp;\\text{if } b \\\\ c \u0026amp;\\text{if } d \\end{cases} {{\u0026lt; /katex \u0026gt;}}     Display Mode Example Here is some inline example: \\(\\pi(x)\\)  , rendered in the same line. And below is display example, having display: block \\[ x = \\begin{cases} a \u0026\\text{if } b \\\\ c \u0026\\text{if } d \\end{cases} \\]  Text continues here.\n"});index.add({'id':14,'href':'/docs/shortcodes/mermaid/','title':"Mermaid",'content':"Mermaid Chart Mermaid is library for generating svg charts and diagrams from text.\nExample {{\u0026lt; mermaid [class=\u0026#34;text-center\u0026#34;]\u0026gt;}} sequenceDiagram Alice-\u0026gt;\u0026gt;Bob: Hello Bob, how are you? alt is sick Bob-\u0026gt;\u0026gt;Alice: Not so good :( else is well Bob-\u0026gt;\u0026gt;Alice: Feeling fresh like a daisy end opt Extra response Bob-\u0026gt;\u0026gt;Alice: Thanks for asking end {{\u0026lt; /mermaid \u0026gt;}}     "});index.add({'id':15,'href':'/docs/shortcodes/tabs/','title':"Tabs",'content':"Tabs Tabs let you organize content by context, for example installation instructions for each supported platform.\n{{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} Example MacOS MacOS This is tab MacOS content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nLinux Linux This is tab Linux content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\nWindows Windows This is tab Windows content.\nLorem markdownum insigne. Olympo signis Delphis! Retexi Nereius nova develat stringit, frustra Saturnius uteroque inter! Oculis non ritibus Telethusa protulit, sed sed aere valvis inhaesuro Pallas animam: qui quid, ignes. Miseratus fonte Ditis conubia.\n "});index.add({'id':16,'href':'/sections/','title':"Sections",'content':""});index.add({'id':17,'href':'/sections/scala-basic/','title':"Scala Basic",'content':"Scala Basics ::: tip Learning Objectives\n Learn how to work with Scala interactive shell. Understand var and val. Define variables, functions and classes, and make function calls Understand Simple Build Tool (SBT).  :::\nIn this section we will briefly go through the essential knowledge about Scala. You will first learn how to work with Scala shell, then learn how to use variables, functions with examples. Finally, we give instructions about how to compile and run a standalone program using sbt.\nScala Shell You can open a Scala shell by typing scala. Or, you can use sbt by typing sbt console. The second approach will help you add your project source code and dependencies into class path, so that your functions or library functions will be available for you to try to in the interactive shell. But in this training, we will stick to Scala shell for simplicity.\nOnce starting the Scala shell you will see\n$ scala Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0). Type in expressions to have them evaluated. Type :help for more information. scala\u0026gt; You can type :quit to stop and quit the shell, but don\u0026rsquo;t do that now :-) Next you will learn some Scala operations in the shell with the following materials.\nVariables Declare val and var In Scala, there are two types of variable, immutable val and mutable var. Unlike some functional programming language that requires immutable variables, Scala allows existence of mutable variables but immutable is recommended as it is easier to verify the correctness of your program. Suppose you are still in the Scala interactive shell. Define an immutable variable as\nscala\u0026gt; val myInt = 1 + 1 myInt: Int = 2 scala\u0026gt; myInt = 3 where val is a keyword in scala that makes the variables immutable. If you reassign a value to myInt, error will be reported.\nscala\u0026gt; myInt = 3 \u0026lt;console\u0026gt;:8: error: reassignment to val myInt = 3 ^ scala\u0026gt; ::: warning\nIn interactive shell, it\u0026rsquo;s possible to redefine variable with same name. In Scala source code file, it\u0026rsquo;s not allowed.\nscala\u0026gt; val a = 1 a: Int = 1 scala\u0026gt; val a = 2 a: Int = 2 :::\nInstead, variables declared with var are mutable. Ideally, we try to use val instead of var if possible as a good practice of functional programming.\n::: tip You may have concern that maybe too many immutable variables will be declared. Actually, with chained function calls, that situation is not the case for well organized code. :::\nAn example of mutable variable is\nscala\u0026gt; var myString = \u0026#34;Hello Big Data\u0026#34; myString: String = Hello Big Data scala\u0026gt; myString = \u0026#34;Hello Healthcare\u0026#34; myString: String = Hello Healthcare Type Scala may seem like a script language like JavaScript or Python, as variable type is not specified explicitly. In fact, Scala is a static type language and the compiler can implicitly infer the type in most cases. However, you can always specify a type as\nscala\u0026gt; val myDouble: Double = 3 myDouble: Double = 3.0 It is always encouraged to specify the type so unless the type is too obvious.\nBesides simple built-in variable types like Int, Double and String, you will also learn about List and Tuple in the training:\nscala\u0026gt; val myList: List[String] = List(\u0026#34;this\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;string\u0026#34;) myList: List[String] = List(this, is, a, list, of, string) scala\u0026gt; val myTuple: (Double, Double) = (1.0, 2.0) myTuple: (Double, Double) = (1.0,2.0) Here the List[String] is syntax of generics in Scala, which is same as C#. In the above example, List[String] means a List of String. Similarly, (Double, Double) means a two-field tuple type and both the 1st element and the 2nd element should be of type Double.\nFunctions You can define a function and invoke the function like\nscala\u0026gt; def triple(x: Int): Int = { x*3 } triple: (x: Int)Int scala\u0026gt; triple(2) res0: Int = 6 Where x: Int is parameter and its type, and the second Int is function return type. There\u0026rsquo;s not explicit return statement, but the result of last expresssion x*3 will be returned (similar to some other programming languages like Ruby). In this example, as there is only one expression and return type can be infered by the compiler, you may define the function as\ndef triple(x: Int) = x*3 Scala is object-oriented (OO), function calls on a class method are straightforward like most OO languages (e.g. Java, C#)\nscala\u0026gt; myString = \u0026#34;Hello Healthcare\u0026#34; myString: String = Hello Healthcare scala\u0026gt; myString.lastIndexOf(\u0026#34;Healthcare\u0026#34;) res1: Int = 6 If the function does not have parameters, you can even call it without parenthesis\nscala\u0026gt; val myInt = 2 scala\u0026gt; myInt.toString res2: String = 2 You can also define an anonymous function and pass it to variable like a lambda expression in some other languages such as Python:\nscala\u0026gt; val increaseOne = (x: Int) =\u0026gt; x + 1 increaseOne: Int =\u0026gt; Int = \u0026lt;function1\u0026gt; scala\u0026gt; increaseOne(3) res3: Int = 4 Anonymous function is very useful as it can be passed as a parameter to a function call\nscala\u0026gt; myList.foreach{item: String =\u0026gt; println(item)} this is a list of string where item: String =\u0026gt; println(item) is an anonymous function. This function call can be further simplified to\nscala\u0026gt; myList.foreach(println(_)) scala\u0026gt; myList.foreach(println) where _ represents first parameter of the anonymous function with body println(_). Additional _ can be specified to represent more than one parameter. For example, we can calculate the total payment that a patient made by\nscala\u0026gt; val payments = List(1, 2, 3, 4, 5, 6) payments: List[Int] = List(1, 2, 3, 4, 5, 6) scala\u0026gt; payments.reduce(_ + _) res0: Int = 21 In above example, reduce will aggregate List[V] into V and we defined the aggregator as _ + _ to sum them up. Of course, you can write that more explicitly like\nscala\u0026gt; payments.reduce((a, b) =\u0026gt; a+b) res1: Int = 21 Here reduce is a construct from functional programming. It can be illustrated with the figure below where a function f is applied to one element at a time and the result together with next element will be parameters of the next function call until the end of the list.\nIt\u0026rsquo;s important to remember that for reduce operation, the input is List[V] and the output is V. Interested reader can learn more from wiki. In contrast to reduce, you can of course write code using for loop, which is verbose and very rare in Scala,\nscala\u0026gt; var totalPayment = 0 totalPayment: Int = 0 scala\u0026gt; for (payment \u0026lt;- payments) { totalPayment += payment } scala\u0026gt; totalPayment res2: Int = 21 scala\u0026gt; Class Declaration of a class in Scala is as simple as\nscala\u0026gt; class Patient(val name: String, val id: Int) defined class Patient scala\u0026gt; val patient = new Patient(\u0026#34;Bob\u0026#34;, 1) patient: Patient = Patient@755f5e80 scala\u0026gt; patient.name res13: String = Bob Here we see the succinct syntax of Scala again. class Patient(val name: String, val id: Int) not only defines constructor of Patient but also defines two member variables (name and id).\nA special kind of class that we will use a lot is Case Class. Case Class can be declared as\nscala\u0026gt; case class Patient(val name: String, val id: Int) and see below Pattern Matching for use case.\nPattern Matching You may know the switch..case in other languages. Scala provides a more flexible and powerful technique, Pattern Matching. Below example shows one can match by-value and by-type in one match.\nval payment:Any = 21 payment match { case p: String =\u0026gt; println(\u0026#34;payment is a String\u0026#34;) case p: Int if p \u0026gt; 30 =\u0026gt; println(\u0026#34;payment \u0026gt; 30\u0026#34;) case p: Int if p == 0 =\u0026gt; println(\u0026#34;zero payment\u0026#34;) case _ =\u0026gt; println(\u0026#34;otherwise\u0026#34;) } It\u0026rsquo;s very convenient to use case class in pattern matching\nscala\u0026gt; val p = new Patient(\u0026#34;Abc\u0026#34;, 1) p: Patient = Patient(Abc,1) scala\u0026gt; p match {case Patient(\u0026#34;Abc\u0026#34;, id) =\u0026gt; println(s\u0026#34;matching id is $id\u0026#34;)} matching id is 1 Here we not only matched p as Patient type, but also matched patient name and extracted one member field from the Patient class instance.\np match { case Patient(\u0026#34;Abc\u0026#34;, id) =\u0026gt; println(s\u0026#34;matching id is $id\u0026#34;) case _ =\u0026gt; println(\u0026#34;not matched\u0026#34;) } Standalone Program Working with large real-world applications, you usually need to compile and package your source code with some tools. Here we show how to compile and run a simple program with sbt. Run the sample code in \u0026lsquo;hello-bigdata\u0026rsquo; folder\n% cd ~/bigdata-bootcamp/sample/hello-bigdata % sbt run Attempting to fetch sbt ######################################################################### 100.0% Launching sbt from sbt-launch-0.13.8.jar [info] ..... [info] Done updating. [info] Compiling 1 Scala source to ./hello-bigdata/target/scala-2.10/classes... [info] Running Hello Hello bigdata [success] Total time: 2 s, completed May 3, 2015 8:42:48 PM The source code file hello.scala is compiled and invoked.\nFurther Reading This is a very brief overview of important features from the Scala language. We highly recommend readers to checkout references below to get a better and more complete understanding of Scala programming language.\n Twitter Scala School Official Scala Tutorial SBT tutorial  "});index.add({'id':18,'href':'/sections/scala-intro/','title':"Scala Intro",'content':"Scala Introduction ::: tip Learning Objectives\n Provide more details of scala language  :::\nBasic Gramma Start using Scala After installed scala, you can type scala in command line and get result as follow:\n$ scala\rWelcome to Scala version 2.11.7\rType in expressions to have them evaluated.\rType :help for more information.\rscala\u0026gt; println(\u0026#34;Hello, World!\u0026#34;)\rHello, World!\rThe synopsis of a varialbe is:\nscala\u0026gt; val i:String = \u0026#34;abc\u0026#34;\ri: String = abc\r val means it it is immutable variable, you can use \u0026ldquo;var\u0026rdquo; to define a mutable variable i is the name of this variable String is the type of this string, it can be omitted here \u0026ldquo;abc\u0026rdquo; is the value of this variable  Define a function:\nscala\u0026gt; def foo(v0:Int, v1:Int):Int = {\r| println(v0 max v1)\r| v0 + v1\r| }\rfoo: (v0: Int, v1: Int)Int\rscala\u0026gt; foo(1, 2)\r2\rres0: Int = 3\rDefine a class:\nscala\u0026gt; class Foo(a:String, b:Int) {\r| def length = a.length\r| }\rdefined class Foo\rscala\u0026gt; val foo:Foo = new Foo(\u0026#34;Hello, World!\u0026#34;, 3)\rfoo: Foo = Foo@6438a396\rscala\u0026gt; println(foo.length)\r13\rDefine a case class:\nscala\u0026gt; case class Foo(a:String, b:Int)\rdefined class Foo\rscala\u0026gt; val foo:Foo = Foo(a = \u0026#34;Hello, World!\u0026#34;, b = 3)\rfoo: Foo = Foo(Hello, World!,3)\rscala\u0026gt; println(foo.a)\rHello, World!\rDifferences between case class and class\nDefine an Object\nscala\u0026gt; object Foo {\r| def greeting() {\r| println(\u0026#34;Greeting from Foo\u0026#34;)\r| }\r| }\rdefined object Foo\rscala\u0026gt; Foo.greeting()\rGreeting from Foo\rFunctions/variables in Object is similar to the static function and variable in Java.\nWhat is ought to be highligted is the use of \u0026ldquo;apply\u0026rdquo;. SomeObject.apply(v:Int) equals SomeObject(v:Int)\nscala\u0026gt; :paste\r// Entering paste mode (ctrl-D to finish)\r\rcase class Foo(a:String, b:Int)\robject Foo {\rdef apply(a:String): Foo =\rFoo(a, a.length)\r}\r// Exiting paste mode, now interpreting.\r\rdefined class Foo\rdefined object Foo\rscala\u0026gt; val foo = Foo(\u0026#34;Hello, World!\u0026#34;)\rfoo: Foo = Foo(Hello, World!,13)\rFinally, we should know the usage of code block.\nWe can create a code block anywhere, and the last line is the result of this block.\nFor example,\ndef foo(i:Int) = {\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r}\rval newList = List[Int](1, 2, 3).map(i =\u0026gt; foo(i))\rWe can use the follow lines instead:\nval newList = List[Int](1, 2, 3).map(i =\u0026gt; {\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r})\rA better practice here is:\nval newList = List[Int](1, 2, 3).map{i =\u0026gt;\rprintln(s\u0026#34;value: $i\u0026#34;)\ri * 2\r}\rCase Study of some Common Types Option, Some, None We can use null in Scala as null pointer, but it is not recommended. We are supposed to use Option[SomeType] to indicate this variable is optional.\nWe can assueme every variable without Option are not null pointer if we are not calling java code. This help us reduce a lot of code.\nIf we wanna to get a variable with Option, here are two method\nval oi = Option(1)\rval i = oi match {\rcase Some(ri) =\u0026gt; ri\rcase None =\u0026gt; -1\r}\rprintln(i)\rBesides, we can also use method \u0026ldquo;isDefined/isEmpty\u0026rdquo;.\nval oi = Option(1)\rif(oi.isDefined) {\rprintln(s\u0026#34;oi: ${oi.get}\u0026#34;)\r} else {\rprintln(\u0026#34;oi is empty\u0026#34;)\r}\rWhat should be highlighted is Option(null) returns None, but Some(null) is Some(null) which is not equals None.\nmatch is a useful reserved words, we can use it in various of situations\nFirstly, we can use it as \u0026ldquo;switch\u0026rdquo; \u0026amp; \u0026ldquo;case\u0026rdquo; in some other programming languages.\ntrue match {\rcase true =\u0026gt; println(\u0026#34;true\u0026#34;)\rcase false =\u0026gt; println(\u0026#34;false\u0026#34;)\r}\rSecondly, we can find it by the type of the data,\ncase class A(i:Int,j:Double)\rcase class B(a:A, k:Double)\rval data = B(A(1,2.0),3.14)\rdata match {\rcase B(A(_,v),_) =\u0026gt;\rprintln(s\u0026#34;required value: $v\u0026#34;)\rcase _ =\u0026gt;\rprintln(\u0026#34;match failed\u0026#34;)\r}\rGiven a case class B, but we only wish to retrievee the value B.a.j, we can use \u0026ldquo;_\u0026rdquo; as placeholder.\nCommon methods in List, Array, Set, and so on In scala, we always transfer the List( Array, Set, Map etc.) from one status to another status. the methods of\n  toList, toArray, toSet \u0026ndash; convert each other\n  par \u0026ndash; Parallelize List, Array and Map, the result of Seq[Int]().par is ParSeq[Int], you will able to process each element in parallel when you are using foreach, map etc., and unable to call \u0026ldquo;sort\u0026rdquo; before you are using \u0026ldquo;toList\u0026rdquo;.\n  distinct \u0026ndash; Removes duplicate elements\n  foreach \u0026ndash; Process each element and return nothing\n  List[Int](1,2,3).foreach{ i =\u0026gt;\rprintln(i)\r}\rIt wil print 1, 2, 3 in order\nList[Int](1,2,3).par.foreach{ i =\u0026gt;\rprintln(i)\r}\rIt will print 1, 2, 3, but the order is not guaranteed.\n map \u0026ndash; Process each element and construct a List using return value  List[Int](1,2,3).map{ i =\u0026gt; i + 1}\rIt will return List[Int](2,3,4)\nThe result of List[A]().map(some-oper-return-type-B) is List[B], while the result of Array[A]().map map is Array[B].\n flatten \u0026ndash; The flatten method takes a list of lists and flattens it out to a single list:  scala\u0026gt; List[List[Int]](List(1,2),List(3,4)).flatten\rres1: List[Int] = List(1, 2, 3, 4)\rscala\u0026gt; List[Option[Integer]](Some(1),Some[Integer](null),Some(2),None,Some(3)).flatten\rres2: List[Integer] = List(1, null, 2, 3)\r  flatMap \u0026ndash; The flatMap is similar to map, but it takes a function returning a list of elements as its right operand. It applies the function to each list element and returns the concatenation of all function results. The result equals to map + flatten\n  collect \u0026ndash; The iterator obtained from applying the partial function to every element in it for which it is defined and collecting the results.\n  scala\u0026gt; List(1,2,3.4,\u0026#34;str\u0026#34;) collect {\r| case i:Int =\u0026gt; (i * 2).toString\r| case f:Double =\u0026gt; f.toString\r| }\rres0: List[String] = List(2, 4, 3.4)\rThe function match elements in Int and Double, process them and return the value, but ignore string elements.\n filter \u0026ndash; Filter this list  scala\u0026gt; List(1,2,3).filter(_ % 2 == 0)\rres1: List[Int] = List(2)\r filterNot \u0026ndash; Similar to filter  scala\u0026gt; List(1,2,3).filterNot(_ % 2 == 0)\rres2: List[Int] = List(1, 3)\r forall \u0026ndash; Return true if All elements are return true by the partial function. It will immediately return once one element returns false, and ignore the rest elements.  scala\u0026gt; List(2,1,0,-1).forall{ i =\u0026gt;\r| val res = i \u0026gt; 0\r| println(s\u0026#34;$i\u0026gt; 0? $res\u0026#34;)\r| res\r| }\r2 \u0026gt; 0? true\r1 \u0026gt; 0? true\r0 \u0026gt; 0? false\rres0: Boolean = false\r exists \u0026ndash; Return true if there are at least One element returns true.  scala\u0026gt; List(2,1,0,-1).exists{ i =\u0026gt;\r| val res = i \u0026lt;= 0\r| println(s\u0026#34;$i\u0026lt;= 0? $res\u0026#34;)\r| res\r| }\r2 \u0026lt;= 0? false\r1 \u0026lt;= 0? false\r0 \u0026lt;= 0? true\rres2: Boolean = true\r find \u0026ndash; Return the first element returns true by the partial function. Return None if no elemet found.  scala\u0026gt; List(2,1,0,-1).find{ i =\u0026gt;\r| val res = i \u0026lt;= 0\r| println(s\u0026#34;$i\u0026lt;= 0? $res\u0026#34;)\r| res\r| }\r2 \u0026lt;= 0? false\r1 \u0026lt;= 0? false\r0 \u0026lt;= 0? true\rres3: Option[Int] = Some(0)\r sortWith \u0026ndash; sort the elements  scala\u0026gt; List(1,3,2).sortWith((leftOne,rightOne) =\u0026gt; leftOne \u0026gt; rightOne)\rres5: List[Int] = List(3, 2, 1)\r zipWithIndex \u0026ndash;  List(\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;).zipWithIndex.foreach{ kv:(String,Int) =\u0026gt; println(s\u0026#34;k:${kv._1}, v:${kv._2}\u0026#34;)}\rIt will rebuild a List with index\nk:a, v:0\rk:b, v:1\r for - Scala\u0026rsquo;s keyword for can be used in various of situations.  Basically,\nfor{\ri \u0026lt;- List(1,2,3)\r} yield (i,i+1)\rIt equals:\nList(1,2,3).map(i =\u0026gt; (i, i+1))\rBesides,\nfor{\ri \u0026lt;- List(1,2,3)\rj \u0026lt;- List(4,5,6)\r} yield (i,j)\rWe will get the cartesian product of List(1,2,3) and List(4,5,6): List((1,4), (1,5), (1,6), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6))\nWe can add a filter in condition:\nfor{\ri \u0026lt;- List(1,2,3)\rif i != 1\rj \u0026lt;- List(4,5,6)\rif i * j % 2 == 1\r} yield (i,j)\rthe result is : List((3,5))\nAnother usage of for is as follow:\nLet\u0026rsquo;s define variables as follow:\nval a = Some(1)\rval b = Some(2)\rval c = Some(3)\rWe can execute like this:\nfor {\ri \u0026lt;- a\rj \u0026lt;- b\rk \u0026lt;- c\rr \u0026lt;- {\rprintln(s\u0026#34;i: $i, j:$j, k:$k\u0026#34;)\rSome(i * j * k)\r}\r} yield r\rThe response is:\ni: 1, j:1, k:3\rres9: Option[Int] = Some(6)\rLet\u0026rsquo;s define b as None:\nscala\u0026gt; val b:Option[Int] = None\rb: Option[Int] = None\rscala\u0026gt; for {\r| i \u0026lt;- a\r| j \u0026lt;- b\r| k \u0026lt;- c\r| r \u0026lt;- {\r| println(s\u0026#34;i: $i, j:$j, k:$k\u0026#34;)\r| Some(i * j * k)\r| }\r| } yield r\rres14: Option[Int] = None\r while - Similar to while in java  var i = 0\rwhile ({\ri = i + 1\ri \u0026lt; 1000\r}){\r// body of while\r\tprintln(s\u0026#34;i: $i\u0026#34;)\r}\r  to, until — (1 to 10) will generate a Seq, with the content of (1,2,3,4…10), (0 until 10) will generate a sequence from 0 to 9. With some test, (0 until 1000).map(xxx) appears to be slower than var i=0; while( i \u0026lt; 1000) { i += 1; sth. else}, but if the body of map is pretty heavy, this cost can be ignored.\n  headOption - Get the head of one list, return None if this list is empty\n  head - Get the head of one list, throw exception if this list is empty\n  take \u0026ndash; Get first at most N elements. (from left to right)\n  scala\u0026gt; List(1,2,3).take(2)\rres0: List[Int] = List(1, 2)\rscala\u0026gt; List(1,2).take(3)\rres1: List[Int] = List(1, 2)\r drop \u0026ndash; Drop first at most N elements.  scala\u0026gt; List(1,2,3).drop(2)\rres2: List[Int] = List(3)\rscala\u0026gt; List(1,2).drop(3)\rres3: List[Int] = List()\rdropRight will drop elements from right to left.\n slice \u0026ndash; Return list in [start-offset, end-offset)  scala\u0026gt; List(1,2,3).slice(1,2)\rres7: List[Int] = List(2)\rscala\u0026gt; List(1,2,3).slice(2,2)\rres8: List[Int] = List()\rval offset = 1\rval size = 3\rList(1,2,3,4,5).slice(offset, size + offset)\rIf the end-offset is greater than the length of this list, it will not throw exception.\n splitAt \u0026ndash; Split this list into two from offset i  scala\u0026gt; List(1,2,3).splitAt(1)\rres10: (List[Int], List[Int]) = (List(1),List(2, 3))\r groupBy \u0026ndash; Partitions a list into a map of collections according to a discriminator function  scala\u0026gt; List(1,2,3).groupBy(i =\u0026gt; if(i % 2 == 0) \u0026#34;even\u0026#34; else \u0026#34;odd\u0026#34; )\rres11: scala.collection.immutable.Map[String,List[Int]] = Map(odd -\u0026gt; List(1, 3), even -\u0026gt; List(2))\r partition \u0026ndash; Splits a list into a pair of collections; one with elements that satisfy the predicate, the other with elements that do not, giving the pair of collections (xs filter p, xs.filterNot p).  scala\u0026gt; List(1,2,3).partition(_ % 2 == 0)\rres12: (List[Int], List[Int]) = (List(2),List(1, 3))\r grouped \u0026ndash; The grouped method chunks its elements into increments.  scala\u0026gt; List(1,2,3,4,5).grouped(2)\rres13: Iterator[List[Int]] = Iterator(List(1, 2), List(3, 4), List(5))\rYou can visit this PDF for an official guide.\nWe also highly recomended to read the book Programming in Scala for more detail instruction.\n"});index.add({'id':19,'href':'/sections/spark-application/','title':"Spark Application",'content':"Spark Application ::: tip Learning Objectives\n Prepare data for machine learning applications. Save/load constructed data to external storage.  :::\nIn this section, we will show how to prepare suitable data for building predictive models to predict heart failure (HF). We will first briefly introduce data types involved. Then we show how to construct training/testing samples from the input data using Spark. Finally we will export data in suitable format for modeling later.\nData Types For many machine learning tasks, such as classification, regression, and clustering, a data point is often represented as a feature vector. Each coordinate of the vector corresponds to a particular feature of the data point.\nFeature Vector MLlib, the machine learning module of Spark, supports two types of vectors: dense and sparse. A dense vector is basically a Double array of length equals to the dimension of the vector. If a vector contains only a few non-zero entries, we can then more efficiently represent the vector by a sparse vector with non-zero indices and the corresponding values only. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\nThe base class of a vector is Vector, and there are two implementations: DenseVector and SparseVector. We recommend using the factory methods implemented in Vectors to create vectors.\nscala\u0026gt; import org.apache.spark.mllib.linalg.{Vector, Vectors} // Create a dense vector (1.0, 0.0, 3.0). scala\u0026gt; val dv = Vectors.dense(1.0, 0.0, 3.0) // Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries. scala\u0026gt; val sv = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) Labeled Point A labeled point is a vector, either dense or sparse, associated with a label/prediction target. In Spark MLlib, labeled points are used as input to supervised learning algorithms. For example, in binary classification like HF prediction, a label should be either 0 or 1. For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, \u0026hellip;. For regression problem like payment prediction, a label is a real-valued number.\nscala\u0026gt; import org.apache.spark.mllib.linalg.Vectors scala\u0026gt; import org.apache.spark.mllib.regression.LabeledPoint // Create a labeled point with label 1 and a dense feature vector. scala\u0026gt; val labeled1 = LabeledPoint(1, Vectors.dense(1.0, 0.0, 3.0)) // Create a labeled point with label 0 and a sparse feature vector. scala\u0026gt; val labeled0 = LabeledPoint(0, Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))) Feature Construction Overview To apply machine learning algorithms, we need to transform our data into RDD[LabeledPoint]. This feature construction is similar to what we did in Hadoop Pig, but will be more concise since we are programming in Scala on Spark. We will need to consider an one-year prediction window. Specifically, we will only use data one year before HF diagnosis. The figure below depicts relationship between prediction window and target.\nWe can also specify an observation window, inside which data will be used to construct feature vectors.\nHigh level steps are depicted as the figure below\nOur parallelization will be on patient level, i.e. each element in RDD is everything about exactly one patient. Feature and prediction target for each patient is almost independent from the others. Recall that our data file is in the following form:\n00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 .... Each line is a 4-tuple (patient-id, event-id, timestamp, value). Suppose now our goal is to predict if a patient will have heart failure. We can use the value associated with the event heartfailure as the label. This value can be either 1.0 (the patient has heart failure) or 0.0 (the patient does not have heart failure). We call a patient with heart failure a positive example or case patient, and a patient without heart failure a negative example or control patient. For example, in the above snippet we can see that patient 00013D2EFD8E45D1 is a positive example. The file case.csv consists of only positive examples, and the file control.csv consists of only negative examples.\nWe will use the values associated with events other than heartfailure to construct feature vector for each patient. Specifically, the length of the feature vector is the number of distinct event-id's, and each coordinate of the vector stores the aggregated value corresponds to a particular event-id. The values associated with events not shown in the file are assume to be 0. Since each patient typically has only a few hundreds of records (lines) compared to thousands of distinct events, it is more efficient to use SparseVector. Note that each patient can have multiple records with the same event-id. In this case we sum up the values associated with a same event-id as feature value and use event-id as feature name.\n1. Load data The file input/case.csv consists of only positive examples, and the file input/control.csv consists of only negative examples. We will load them together. Since the data will be used more than once, we use cache() to prevent reading in the file multiple times.\ncase class Event(patientId: String, eventId: String, timestamp: Int, value: Double) val rawData = sc.textFile(\u0026#34;input/\u0026#34;). map{line =\u0026gt; val splits = line.split(\u0026#34;,\u0026#34;) new Event(splits(0), splits(1), splits(2).toInt, splits(3).toDouble) } 2. Group patient data One patient\u0026rsquo;s index date, prediction target etc are independent from another patient, so that we can group by patient-id to put everything about one patient together. When we run map operation, Spark will help us parallelize computation on patient level.\n// group raw data with patient id and ignore patient id // then we will run parallel on patient lelvel val grpPatients = rawData.groupBy(_.patientId).map(_._2) The groupBy operation can be illustrated with the example below\nPlease recall that _._2 will return second field of a tuple. In this case it will return the List[event] for a given patient. Finally the grpPatients will be RDD[List[event]]\n3. Define target and feature values Now, we can practice our patient level parallelization. For each patient, we first find the prediction target, which is encoded into an event with name heartfailure, then we identify the index date and keep only useful events before the index date for feature construction. In feature construction, we aggregate the event value into features using sum function and use the event name as the feature name.\nval patientTargetAndFeatures = grpPatients.map{events =\u0026gt; // find event that encode our prediction target, heart failure  val targetEvent = events.find(_.eventId == \u0026#34;heartfailure\u0026#34;).get val target = targetEvent.value // filter out other events to construct features  val featureEvents = events.filter(_.eventId != \u0026#34;heartfailure\u0026#34;) // define index date as one year before target  // and use events happened one year before heart failure  val indexDate = targetEvent.timestamp - 365 val filteredFeatureEvents = featureEvents.filter(_.timestamp \u0026lt;= indexDate) // aggregate events into features  val features = filteredFeatureEvents. groupBy(_.eventId). map{case(eventId, grpEvents) =\u0026gt; // user event id as feature name  val featureName = eventId // event value sum as feature value  val featureValue = grpEvents.map(_.value).sum (featureName, featureValue) } (target, features) } The construction of target is relatively simple, but the process of constructing features is tricky. The example below show what happened in main body of above map operation to illustrate how features were constructed\nOur final filteredFeatureEvents should be RDD[(target, Map[feature-name, feature-value])] and we can verify that by the following:\nscala\u0026gt; patientTargetAndFeatures.take(1) res0: Array[(Double, scala.collection.immutable.Map[String,Double])] = Array((0.0,Map(DRUG36987241603 -\u0026gt; 60.0, DRUG00378181701 -\u0026gt; 30.0, DRUG11517316909 -\u0026gt; 20.0, DRUG53002055230 -\u0026gt; 200.0, DRUG23490063206 -\u0026gt; 30.0, DRUG61113074382 -\u0026gt; 60.0, DRUG58016093000 -\u0026gt; 60.0, DRUG52604508802 -\u0026gt; 30.0, DRUG58016037228 -\u0026gt; 10.0, DRUG60491080134 -\u0026gt; 30.0, DRUG51079093119 -\u0026gt; 360.0, DRUG00228275711 -\u0026gt; 30.0, DRUG63629290803 -\u0026gt; 120.0, DIAG4011 -\u0026gt; 1.0, DRUG58016075212 -\u0026gt; 90.0, DRUG00378412401 -\u0026gt; 30.0, DRUG63629260701 -\u0026gt; 30.0, DRUG00839619116 -\u0026gt; 30.0, DRUG11390002315 -\u0026gt; 30.0, DRUG58016058050 -\u0026gt; 60.0, DRUG55289082930 -\u0026gt; 60.0, DRUG36987154502 -\u0026gt; 30.0, DRUG00364095301 -\u0026gt; 30.0, DRUG58016021326 -\u0026gt; 180.0, DRUG54868593401 -\u0026gt; 30.0, DRUG58016054035 -\u0026gt; 30.0, DRUG64464000105 -\u0026gt; 30.0, DRUG58016076573 -\u0026gt; 30.0, DRUG00839710006... 4. Feature name to id In the previous step, we computed filteredFeatureEvents as RDD[(target, Map[feature-name, feature-value])]. In order to convert feature-name to some integer id as required by most machine learning modules including MLlib, we will need to collect all unique feauture names and associate them with integer ids.\n// assign a unique integer id to feature name val featureMap = patientTargetAndFeatures. // RDD[(target, Map[feature-name, feature-value])]  flatMap(_._2.keys). // get all feature names  distinct. // remove duplication  collect. // collect to driver program  zipWithIndex. // assign an integer id  toMap // convert to Map[feature-name, feature-id] Here we used an operation named flatMap. Below is an example, and we can think of flatMap as a two step operation, map and flatten. As a result, patientTargetAndFeatures.flatMap(_._2.keys) will give RDD[feature-name].\nNext we visualize the steps after flatMap:\nHere collect is not depicted but what collect does is to collect data from distributed to centralized storage on the driver. Here we assume the resulting data matrix is not too big. If the data matrix is very big, alternative approach may be required such as join. Note that many the common functions like zipWithIndex have the same name on RDD and on common local data structures like List.\n::: tip\nIf you get confused about result of certain operations, you can avoid chain of operation calls and instead print out the result of each step.\n:::\n5. Create final LabeledPoint In this last step, we transform (target, features) for each patient into LabeledPoint. Basically, we just need to translate feature name in features into feautre id and create a feature vector then associate the vector with target.\n// broadcast feature map from driver to all workers val scFeatureMap = sc.broadcast(featureMap) val finalSamples = patientTargetAndFeatures.map {case(target, features) =\u0026gt; val numFeature = scFeatureMap.value.size val indexedFeatures = features. toList. // map feature name to id to get List[(feature-id, feature-value)]  map{case(featureName, featureValue) =\u0026gt; (scFeatureMap.value(featureName), featureValue)} val featureVector = Vectors.sparse(numFeature, indexedFeatures) val labeledPoint = LabeledPoint(target, featureVector) labeledPoint } Here in above example, we called sc.broadcast. As indicated by its name, this function is used for broadcasting data from driver to workers so that workers will not need to copy on demand and waste bandwidth thus slow down the process. Its usage is very simple, call val broadcasted = sc.broadcast(object) and use broadcasted.value to access original object. Please be aware of the fact that such broadcasted object is read-only.\nSave With data readily available as RDD[LabeledPoint], we can save it into a common format accepted by a lot of machine learning modules, the LibSVM/svmlight format, named after LibSVM/svmlight package.\nimport org.apache.spark.mllib.util.MLUtils MLUtils.saveAsLibSVMFile(finalSamples, \u0026#34;samples\u0026#34;) You can achieve this by\nval mapping = featureMap. toList. sortBy(_._2). map(pair =\u0026gt; s\u0026#34;${pair._1}|${pair._2}\u0026#34;). // intentionally use special seperator  mkString(\u0026#34;\\n\u0026#34;) scala.tools.nsc.io.File(\u0026#34;mapping.txt\u0026#34;).writeAll(mapping) "});index.add({'id':20,'href':'/sections/spark-basic/','title':"Spark Basic",'content':"Spark Basics ::: tip Learning Objectives\n Invoke command in Spark interactive shell. Be familiar with RDD concept. Know basic RDD operations.  :::\nSpark Shell Spark can run in several modes, including YARN client/server, Standalone, Mesos and Local. For this training, we will use local mode. Specifically, you can start the Spark interactive shell by invoking the command below in the terminal to run Spark in the local mode with two threads. Then you will see\n\u0026gt; spark-shell --master \u0026quot;local[2]\u0026quot; --driver-memory 6G Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties ... [messages] ... Spark context available as sc. scala\u0026gt; Here you can set --driver-memory according to your local setting. If your setting of driver memory is larger than the VM memory, don\u0026rsquo;t forget to change the VM memory setting first.\nIn Spark, we call the main entrance of a Spark program the driver and Spark distribute computation to workers to compute. Here in the interactive shell, the Spark shell program is the driver. In above example we set the memory of driver program to 3GB as in local mode driver and worker are together. A driver program can access Spark through a SparkContext object, which represents a connection to a computing cluster. In the above interactive shell, SparkContext is already created for you as variable sc. You can input sc to see its type.\nscala\u0026gt; sc res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@27896d3b You may find the logging statements that get printed in the shell distracting. You can control the verbosity of the logging. To do this, you can create a file in the conf directory called log4j.properties. The Spark developers already include a template for this file called log4j.properties.template. To make the logging less verbose, make a copy of conf/log4j.properties.template called conf/log4j.properties and find the following line:\nlog4j.rootCategory=INFO, console Replace INFO with WARN so that only WARN messages and above are shown.\nRDD Resilient Distributed Dataset (RDD) is Spark\u0026rsquo;s core abstraction for working with data. An RDD is simply a fault-tolerant distributed collection of elements. You can imagine RDD as a large array but you cannot access elements randomly but you can apply the same operations to all elements in the array easily. In Spark, all the work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute results. There are two ways to create RDDs: by distributing a collection of objects (e.g., a list or set), or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\nParallelized Collections For the demo purpose, the simplest way to create an RDD is to take an existing collection (e.g. a Scala Array) in your program and pass it to SparkContext\u0026rsquo;s parallelize() method.\nscala\u0026gt; val data = Array(1, 2, 3, 4, 5) data: Array[Int] = Array(1, 2, 3, 4, 5) scala\u0026gt; val distData = sc.parallelize(data) distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at \u0026lt;console\u0026gt;:23 Once created, the distributed dataset (distData) can be operated in parallel. For example, we can add up the elements by calling distData.reduce((a, b) =\u0026gt; a + b). You will see more operations on RDD later on.\n::: warning\nParallelizing a collection is useful when you are learning Spark. However, this is not encouraged in production since it requires the entire dataset to be in memory of the driver\u0026rsquo;s machine first. Instead, importing data from external datasets should be employed.\n:::\nExternal Datasets A common way for creating RDDs is loading data from external storage. Below you will learn how to load data from a file system. Assuming you have put some data into HDFS as described in the HDFS Basic section. If not, please do that first.\nscala\u0026gt; val lines = sc.textFile(\u0026#34;input/case.csv\u0026#34;) lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at \u0026lt;console\u0026gt;:21 Here in the above example, each line of the original file will become an element in the lines RDD.\n::: tip Reading data from a file system, Spark relies on the HDFS library. In above example we assume HDFS is well configured through environmental variables or configuration files so that data is ready in HDFS. :::\nRDD Operations RDDs offer two types of operations: transformations and actions:\n Transformations are operations on RDDs that return a new RDD, such as map() and filter(). Actions are operations that return a result to the driver program or write it to storage, such as first() and count().  Spark treats transformations and actions very differently, so understanding which type of operation you are performing is very important. You can check whether a function is a transformation or an action by looking at its return type: transformations return RDDs, whereas actions return some other data type.\nAll transformations in Spark are lazy, in that they do not compute the results right away. Instead, they just remember the operations applied to some base dataset (e.g. an Array or a file). The transformations are only computed when an action requires a result to be returned to the driver program. Therefore, the above command of reading in a file has not actually been executed yet. We can force the evaluation of RDDs by calling any actions.\nLet\u0026rsquo;s go through some common RDD operations using the healthcare dataset. Recall that in the file case.csv, each line is a 4-field tuple (patient-id, event-id, timestamp, value).\nCount In order to know how large is our raw event sequence data, we can count the number of lines in the input file using count operation, i.e.\nscala\u0026gt; lines.count() res1: Long = 14046 Clearly, count is an action.\nTake You may wonder what the loaded data looks like, you can take a peek at the data. The take(k) will return the first k elements in the RDD. Spark also provides collect() which brings all the elements in the RDD back to the driver program. Note that collect() should only be used when the data is small. Both take and collect are actions.\nscala\u0026gt; lines.take(5) res2: Array[String] = Array(00013D2EFD8E45D1,DIAG78820,1166,1.0, 00013D2EFD8E45D1,DIAGV4501,1166,1.0, 00013D2EFD8E45D1,heartfailure,1166,1.0, 00013D2EFD8E45D1,DIAG2720,1166,1.0, 00013D2EFD8E45D1,DIAG4019,1166,1.0) We got the first 5 records in this RDD. However, this is hard to read due to a poor format. We can make it more readable by traversing the array to print each record on its own line.\nscala\u0026gt; lines.take(5).foreach(println) 00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 00013D2EFD8E45D1,DIAG4019,1166,1.0 Note that in above 3 code block examples, the RDD lines has been computed (i.e. read in from file) 3 times. We can prevent this by calling lines.cache(), which will cache the RDD in memory to avoid reloading.\nscala\u0026gt; lines.take(5).map(_.split(\u0026#34;,\u0026#34;)).map(_(1)).foreach(println) Map The map operation in Spark is similar to that of Hadoop. It\u0026rsquo;s a transformation that transforms each item in the RDD into a new item by applying the provided function. Notice this map will map exactly one element from source to target. For example, suppose we are only interested in knowing IDs of patients, we use map like\nscala\u0026gt; lines.map(line =\u0026gt; line.split(\u0026#34;,\u0026#34;)(0)) It is also possible to write a more complex, multiple-lines map function. In this case, curly braces should be used in place of parentheses. For example, we can get both patient-id and event-id as a tuple at the same time.\nscala\u0026gt; lines.map{line =\u0026gt; val s = line.split(\u0026#34;,\u0026#34;) (s(0), s(1)) } Filter As indicated by its name, filter can transform an RDD to another RDD by keeping only elements that satisfy the filtering condition. For example, we want to count the number of events collected for a particular patient to verify amount of the data from that patient. We can use a filter function.\nscala\u0026gt; lines.filter(line =\u0026gt; line.contains(\u0026#34;00013D2EFD8E45D1\u0026#34;)).count() res4: Long = 200 Distinct distinct is a transformation that transforms a RDD to another by eliminating duplications. We can use that to count the number of distinct patients. In order to do this, we first extract the patient ID from each line. We use the map() function as described above. In this example, we transform each line into the corresponding patient ID by extracting only the first column. We then eliminate duplicate IDs by the distinct() function.\nscala\u0026gt; lines.map(line =\u0026gt; line.split(\u0026#34;,\u0026#34;)(0)).distinct().count() res5: Long = 100 Group Sometimes, you will need to group the input events according to patient-id to put everything about each patient together. For example, in order to extract index date for predictive modeling, you may first group input data by patient then handle each patient seperately in parallel. We can see each element in RDD is a (Key, Value) pair (patient-id, iterable[event]).\n\u0026gt; val patientIdEventPair = lines.map{line =\u0026gt; val patientId = line.split(\u0026#34;,\u0026#34;)(0) (patientId, line) } \u0026gt; val groupedPatientData = patientIdEventPair.groupByKey \u0026gt; groupedPatientData.take(1) res1: Array[(String, Iterable[String])] = Array((0102353632C5E0D0,CompactBuffer(0102353632C5E0D0,DIAG29181,562,1.0, 0102353632C5E0D0,DIAG29212,562,1.0, 0102353632C5E0D0,DIAG34590,562,1.0, 0102353632C5E0D0,DIAG30000,562,1.0, 0102353632C5E0D0,DIAG2920,562,1.0, 0102353632C5E0D0,DIAG412,562,1.0, 0102353632C5E0D0,DIAG28800,562,1.0, 0102353632C5E0D0,DIAG30391,562,1.0, 0102353632C5E0D0,DIAGRG894,562,1.0, 0102353632C5E0D0,PAYMENT,562,6000.0, 0102353632C5E0D0,DIAG5781,570,1.0, 0102353632C5E0D0,DIAG53010,570,1.0, 0102353632C5E0D0,DIAGE8490,570,1.0, 0102353632C5E0D0,DIAG27651,570,1.0, 0102353632C5E0D0,DIAG78559,570,1.0, 0102353632C5E0D0,DIAG56210,570,1.0, 0102353632C5E0D0,DIAG5856,570,1.0, 0102353632C5E0D0,heartfailure,570,1.0, 0102353632C5E0D0,DIAG5070,570,1.0, 0102353632C5E0D0,DIAGRG346,570,1.0,... .... Reduce By Key reduceByKey transforms an RDD[(K, V)] into RDD[(K, List[V])] (like what groupByKey does) and then apply reduce function on List[V] to get final output RDD[(K, V)]. Please be careful that we intentionally denote V as return type of reduce which should be same as input type of the list element. Suppose now we want to calculate the total payment by each patient. A payment record in the dataset is in the form of (patient-id, PAYMENT, timestamp, value).\nval payment_events = lines.filter(line =\u0026gt; line.contains(\u0026#34;PAYMENT\u0026#34;)) val payments = payment_events.map{ x =\u0026gt; val s = x.split(\u0026#34;,\u0026#34;) (s(0), s(3).toFloat) } val paymentPerPatient = payments.reduceByKey(_+_) The payment_events RDD returned by filter contains those records associated with payment. Each item is then transformed to a key-value pair (patient-id, payment) with map. Because each patient can have multiple payments, we need to use reduceByKey to sum up the payments for each patient. Here in this example, patient-id will be served as the key, and payment will be the value to sum up for each patient. The figure below shows the process of reduceByKey in our example\nSort We can then find the top-3 patients with the highest payment by using sortBy first.\nscala\u0026gt; paymentPerPatient.sortBy(_._2, false).take(3).foreach(println) and output is\n(0085B4F55FFA358D,139880.0) (019E4729585EF3DD,108980.0) (01AC552BE839AB2B,108530.0) Again in sortBy we use the _ placeholder, so that _._2 is an anonymous function that returns the second element of a tuple, which is the total payment a patient. The second parameter of sortBy controls the order of sorting. In above example, false means decreasing order.\nscala\u0026gt; val maxPaymentPerPatient = payments.reduceByKey(math.max) Here, reduceByKey(math.max) is the simplified expression of reduceByKey(math.max(_,_)) or reduceByKey((a,b) =\u0026gt; math.max(a,b)). math.max is a function in scala that turns the larger one of two parameters.\n\u0026lt;ExerciseComponent question=\u0026quot;Count the number of records for each drug (event-id starts with \u0026ldquo;DRUG\u0026rdquo;)\u0026rdquo; answer=\u0026quot;\u0026ldquo;\u0026gt;\nscala\u0026gt; val drugFrequency = lines.filter(_.contains(\u0026#34;DRUG\u0026#34;)). map{ x =\u0026gt; val s = x.split(\u0026#34;,\u0026#34;) (s(1), 1) }.reduceByKey(_+_) Statistics Now we have total payment information of patients, we can run some basic statistics. For RDD consists of numeric values, Spark provides some useful statistical primitives.\nscala\u0026gt; val payment_values = paymentPerPatient.map(payment =\u0026gt; payment._2).cache() scala\u0026gt; payment_values.max() res6: Float = 139880.0 scala\u0026gt; payment_values.min() res7: Float = 3910.0 scala\u0026gt; payment_values.sum() res8: Double = 2842480.0 scala\u0026gt; payment_values.mean() res9: Double = 28424.8 scala\u0026gt; payment_values.stdev() res10: Double = 26337.091771112468 Set Operation RDDs support many of the set operations, such as union and intersection, even when the RDDs themselves are not properly sets. For example, we can combine the two files by the union function. Please notice that union here is not strictly identical to union operation in mathematics as Spark will not remove duplications.\nscala\u0026gt; val linesControl = sc.textFile(\u0026#34;input/control.csv\u0026#34;) scala\u0026gt; lines.union(linesControl).count() res11: Long = 31144 ::: tip\nHere, a more straightforward way is to use directory name to read in multiple files of that directory into a single RDD.\nscala\u0026gt; val lines = sc.textFile(\u0026#34;input/\u0026#34;) :::\nscala\u0026gt; val drugCase = sc.textFile(\u0026#34;input/case.csv\u0026#34;). filter(_.contains(\u0026#34;DRUG\u0026#34;)). map(_.split(\u0026#34;,\u0026#34;)(1)). distinct() scala\u0026gt; val drugControl = sc.textFile(\u0026#34;input/control.csv\u0026#34;). filter(_.contains(\u0026#34;DRUG\u0026#34;)). map(_.split(\u0026#34;,\u0026#34;)(1)). distinct() scala\u0026gt; drugCase.intersection(drugControl).count() res: Long = 396 Datasets ::: warning\n This section is working in porgress Dataset is added from Spark 1.6+  :::\nA Dataset is a new interface added from Spark 1.6 that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.\nA Dataset has a concrete type of a Scala primitive type (Integer, Long, Boolean, etc) or a subclass of a Product - a case class. The case class is preferred for Spark because it handles the serialization code for you thus allowing Spark to shuffle data between workers. This can additional be handled implementing Externalizable which is a much more efficient mechanism to handle serialization, or by using a compact serializer like Kryos.\nAdditionally, Spark no longer uses SparkContext directly but prefers the use of a SparkSession that encapsulates a SparkContext and a SqlContext. The SparkSession is a member of the sql package.\nThere is a wealth of great documentation on the Spark development site.\nCreation of Datasets Datasets can be created explicitly or loaded form a source (e.g. file, stream, parquet, etc).\ncase class Person(firstName: String, lastName:String) // wire-in spark implicits import spark.implicits._ case class Person(firstName: String, lastName: String) val ds = Seq(Person(\u0026#34;Daniel\u0026#34;, \u0026#34;Williams\u0026#34;)).toDS() // here you can perform operations that are deferred until an action is invoked. // creates a anonymous lambda that looks at the // firstName of the Dataset[Person] type and invokes a collect // to pull data back to the driver as an Array[Person] // the foreach then will invoke a println on each Person // instance and implicit apply the toString operation that is // held in the Product trait ds.filter(_.firstName == \u0026#34;Daniel\u0026#34;).collect().foreach(println)  Spark SQL, DataFrames and Datasets Guide  Further Reading For the complete list of RDD operations, please see the Spark Programming Guide.\n"});index.add({'id':21,'href':'/sections/spark-gotchas/','title':"Spark Gotchas",'content':"Spark Gotchas ::: tip Learning Objectives\n Understanding runtime pitfalls that may arise  :::\n::: danger\nThis doc is required to be updated.\n:::\nOverview Spark has a handful of gotchas that exist while running in any clustered environment (standalone, K8s, Mesos, Yarn) which can cause runtime issues late in the execution of a job depending how it is coded. It is better to know of these pitfalls ahead of time as the scala compiler will not alert you of them as syntactically your code may be perfect.\nMember Serialization\nThe following is a remarkably unrealistic example as why would you not just create the variable in the main?.\nHowever, there are instances when sub-classing that you may want to override values that are a member of the base. In that, it should be known serialization issues can arise. Spark serializes entire objects with their initial state to workers but deal with implicits differently \u0026ndash; it\u0026rsquo;s always better to declare and initialize variables that will not change locally in the call stack from main().\nNote: Spark uses closures to defined execution strategies but these closures do not operate in the same fashion as in Scala itself.\ncase class Test(x: String) object ObjectMemberIssue { // When foo is accessed via a map operation (transformation) on a worker \t// it will be null \tvar foo: String = null def main(args: Array[String]) : Unit = { val session = SparkSession.builder().appName(\u0026#34;foo\u0026#34;).getOrCreate() foo = \u0026#34;bar\u0026#34; import session.implicits._ val dsOfStrings = (1 to 100).map(new Test(_.toString)).toDS() // This will produce a NullPointerException on the worker which will \t// cause it to die if the job is not running in local mode in the same JVM \t// as the driver. In cluster mode (standalone or otherwise) MemberIssue \t// will be serialized in its default state to the worker causing \t// the access of foo and the append to x to produce the NPE \tdsOfStrings.map(i =\u0026gt; i.copy(i.x ++ foo)).collect().foreach(println) } } An alternative solution is to use an implicit val:\nobject ObjectMemberIssueSolution { var foo: String = null // implicits are searched for and are contexually bound for serialization \t// at runtime time but are intrinsically value structures and will not change \tdef append(base: String)(implicit val s: String) = base ++ s def main(args: Array[String]) : Unit = { val session = SparkSession.builder().appName(\u0026#34;foo\u0026#34;).getOrCreate() foo = \u0026#34;bar\u0026#34; // implicits are contexual serialized \t// especially when passed through another method which forces it to be bound \t// in the Spark serializer as the scala compiler guarantees implicits are bound \t// at compile time based upon references \timplicit val _foo = foo import session.implicits._ val dsOfStrings = (1 to 100).map(new Test(_.toString)).toDS() // This will produce a NullPointerException on the worker which will \t// cause it to die if the job is not running in local mode in the same JVM \t// as the driver. In cluster mode (standalone or otherwise) MemberIssue \t// will be serialized in its default state to the worker but b/c the \t// _foo is an implicit val prior to access subsequent invocations of append will not \t// cause issue. \t// \t// The implicit does not need to be passed as it is automagically wired in via \t// \tdsOfStrings.map(i =\u0026gt; i.copy(append(i.x)).collect().foreach(println) } } Accumulators\n are write only on the worker (unless running in local mode in the same JVM) only the driver can read a value of an accumulator or reset the value of the accumlator  Broadcast variables\n are meant to serialize large amounts of data to each worker for fast access. This data should not change frequently and be relatively static. data can be reset via unpersist(blocking=True) which will force each worker to delete its cached version of the broadcast variable but the broadcast will stay in memory for the remaining stage execution. Upon the next stage execution on the worker the updated state of the broadcast variable will be re-transmitted. ** an elegant way to handle the eviction of Broadcast data (or accessing of accumulators) is by registering a SparkListener implementation with the SparkSession  "});index.add({'id':22,'href':'/sections/spark-graphx/','title':"Spark Graphx",'content':"Spark GraphX ::: tip Learning Objectives\n Understand composition of a graph in Spark GraphX. Being able to create a graph. Being able to use the built-in graph algorithm.  :::\nIn this section we begin by creating a graph with patient and diagnostic codes. Later we will show how to run graph algorithms on the the graph you will create.\nBasic concept Spark GraphX abstracts a graph as a concept named Property Graph, which means that each edge and vertex is associated with some properties. The Graph class has the following definition\nclass Graph[VD, ED] { val vertices: VertexRDD[VD] val edges: EdgeRDD[ED] } Where VD and ED define property types of each vertex and edge respectively. We can regard VertexRDD[VD] as RDD of (VertexID, VD) tuple and EdgeRDD[ED] as RDD of (VertexID, VertexID, ED).\nGraph construction Let\u0026rsquo;s create a graph of patients and diagnostic codes. For each patient we can assign its patient id as vertex property, and for each diagnostic code, we will use the code as vertex property. For the edge between patient and diagnostic code, we will use number of times the patient is diagnosed with given disease as edge property.\nDefine class Let\u0026rsquo;s first define necessary data structure and import\nimport org.apache.spark.SparkContext._ import org.apache.spark.graphx._ import org.apache.spark.rdd.RDD abstract class VertexProperty extends Serializable case class PatientProperty(patientId: String) extends VertexProperty case class DiagnosticProperty(icd9code: String) extends VertexProperty case class PatientEvent(patientId: String, eventName: String, date: Int, value: Double) Load raw data Load patient event data and filter out diagnostic related events only\nval allEvents = sc.textFile(\u0026#34;data/\u0026#34;). map(_.split(\u0026#34;,\u0026#34;)). map(splits =\u0026gt; PatientEvent(splits(0), splits(1), splits(2).toInt, splits(3).toDouble)) // get and cache diagnosticEvents as we will reuse val diagnosticEvents = allEvents. filter(_.eventName.startsWith(\u0026#34;DIAG\u0026#34;)).cache() Create vertex Patient vertex Let\u0026rsquo;s create patient vertex\n// create patient vertex val patientVertexIdRDD = diagnosticEvents. map(_.patientId). distinct. // get distinct patient ids  zipWithIndex // assign an index as vertex id  val patient2VertexId = patientVertexIdRDD.collect.toMap val patientVertex = patientVertexIdRDD. map{case(patientId, index) =\u0026gt; (index, PatientProperty(patientId))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] In order to use the newly created vertex id, we finally collect all the patient to VertrexID mapping.\n::: warning\nTheoretically, collecting RDD to driver is not an efficient practice. One can obtain uniqueness of ID by calculating ID directly with a Hash.\n:::\nDiagnostic code vertex Similar to patient vertex, we can create diagnostic code vertex with\n// create diagnostic code vertex val startIndex = patient2VertexId.size val diagnosticVertexIdRDD = diagnosticEvents. map(_.eventName). distinct. zipWithIndex. map{case(icd9code, zeroBasedIndex) =\u0026gt; (icd9code, zeroBasedIndex + startIndex)} // make sure no conflict with patient vertex id  val diagnostic2VertexId = diagnosticVertexIdRDD.collect.toMap val diagnosticVertex = diagnosticVertexIdRDD. map{case(icd9code, index) =\u0026gt; (index, DiagnosticProperty(icd9code))}. asInstanceOf[RDD[(VertexId, VertexProperty)]] Here we assign vertex id by adding the result of zipWithIndex with an offset obtained from previous patient vertex to avoid ID confliction between patient and diagnostic code.\nCreate edge In order to create edge, we will need to know vertext id of vertices we just created.\nval bcPatient2VertexId = sc.broadcast(patient2VertexId) val bcDiagnostic2VertexId = sc.broadcast(diagnostic2VertexId) val edges = diagnosticEvents. map(event =\u0026gt; ((event.patientId, event.eventName), 1)). reduceByKey(_ + _). map{case((patientId, icd9code), count) =\u0026gt; (patientId, icd9code, count)}. map{case(patientId, icd9code, count) =\u0026gt; Edge( bcPatient2VertexId.value(patientId), // src id  bcDiagnostic2VertexId.value(icd9code), // target id  count // edge property  )} We first broadcast patient and diagnostic code to vertext id mapping. Broadcast can avoid unnecessary copy in distributed setting thus will be more effecient. Then we count occurrence of (patient-id, icd-9-code) pairs with map and reduceByKey, finally we translate them to proper VertexID.\nAssemble vertex and edge We will need to put vertices and edges together to create the graph\nval vertices = sc.union(patientVertex, diagnosticVertex) val graph = Graph(vertices, edges) Graph operation Given the graph we created, we can run some basic graph operations.\nConnected components Connected component can help find disconnected subgraphs. GraphX provides the API to get connected components as below\nval connectedComponents = graph.connectedComponents The return result is a graph and assigned components of original graph is stored as VertexProperty. For example\nscala\u0026gt; connectedComponents.vertices.take(5) Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((2556,0), (1260,0), (1410,0), (324,0), (180,0)) The first element of the tuple is VertexID identical to original graph. The second element in the tuple is connected component represented by the lowest-numbered VertexID in that component. In above example, five vertices belong to same component.\nWe can easily get number of connected components using operations on RDD as below.\nscala\u0026gt; connectedComponents.vertices.map(_._2).distinct.collect Array[org.apache.spark.graphx.VertexId] = Array(0, 169, 239) Degree The property graph abstraction of GraphX is a directed graph. It provides computation of in-dgree, out-degree and total degree. For example, we can get degrees as\nval inDegrees = graph.inDegrees val outDegrees = graph.outDegrees val totalDegrees = graph.degrees PageRank GraphX also provides implementation of the famous PageRank algorithm, which can compute the \u0026lsquo;importance\u0026rsquo; of a vertex. The graph we generated above is a bipartite graph and not suitable for PageRank. To gve an example of PageRank, we randomly generate a graph and run fixed iteration of PageRank algorithm on it.\nimport org.apache.spark.graphx.util.GraphGenerators val randomGraph:Graph[Long, Int] = GraphGenerators.logNormalGraph(sc, numVertices = 100) val pagerank = randomGraph.staticPageRank(20) Or, we can run PageRank until converge with tolerance as 0.01 using randomGraph.pageRank(0.01)\nApplication Next, we show some how we can ultilize the graph operations to solve some practical problems in the healthcare domain.\nExplore comorbidities Comorbidity is additional disorders co-occuring with primary disease. We know all the case patients have heart failure, we can explore possible comorbidities as below (see comments for more explaination)\n// get all the case patients val casePatients = allEvents. filter(event =\u0026gt; event.eventName == \u0026#34;heartfailure\u0026#34; \u0026amp;\u0026amp; event.value == 1.0). map(_.patientId). collect. toSet // broadcast val scCasePatients = sc.broadcast(casePatients) //filter the graph with subGraph operation val filteredGraph = graph.subgraph(vpred = {case(id, attr) =\u0026gt; val isPatient = attr.isInstanceOf[PatientProperty] val patient = if(isPatient) attr.asInstanceOf[PatientProperty] else null // return true iff. isn\u0026#39;t patient or is case patient  !isPatient || (scCasePatients.value contains patient.patientId) }) //calculate indegrees and get top vertices val top5ComorbidityVertices = filteredGraph.inDegrees. takeOrdered(5)(scala.Ordering.by(-_._2)) We have\ntop5ComorbidityVertices: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((3129,86), (335,63), (857,58), (2048,49), (669,48)) And we can check the vertex of index 3129 in original graph is\nscala\u0026gt; graph.vertices.filter(_._1 == 3129).collect Array[(org.apache.spark.graphx.VertexId, VertexProperty)] = Array((3129,DiagnosticProperty(DIAG4019))) The 4019 code correponds to Hypertension, which is reasonable.\nSimilar patients Given a patient diagnostic graph, we can also find similar patients. One of the most straightforward approach is shortest path on the graph.\nval sssp = graph. mapVertices((id, _) =\u0026gt; if (id == 0L) 0.0 else Double.PositiveInfinity). pregel(Double.PositiveInfinity)( (id, dist, newDist) =\u0026gt; math.min(dist, newDist), // Vertex Program  triplet =\u0026gt; { // Send Message  var msg: Iterator[(org.apache.spark.graphx.VertexId, Double)] = Iterator.empty if (triplet.srcAttr + 1 \u0026lt; triplet.dstAttr) { msg = msg ++ Iterator((triplet.dstId, triplet.srcAttr + 1)) } if (triplet.dstAttr + 1 \u0026lt; triplet.srcAttr) { msg = msg ++ Iterator((triplet.srcId, triplet.dstAttr + 1)) } println(msg) msg }, (a,b) =\u0026gt; math.min(a,b) // Merge Message  ) // get top 5 most similar sssp.vertices.filter(_._2 \u0026lt; Double.PositiveInfinity). filter(_._1 \u0026lt; 300). takeOrdered(5)(scala.Ordering.by(-_._2)) "});index.add({'id':23,'href':'/sections/spark-mllib/','title':"Spark Mllib",'content':"Spark MLlib and Scikit-learn ::: tip Learning Objectives\n Understand input to MLlib. Learn to run basic classification algorithms. Learn to export/load trained models. Develop models using python machine learning module.  :::\nIn this section, you will learn how to build a heart failure (HF) predictive model. You should have finished previous Spark Application section. You will first learn how to train a model using Spark MLlib and save it. Next, you will learn how to achieve same goal using Python Scikit-learn machine learning module for verification purpose.\nMLlib You will first load data and compute some high-level summary statistics, then train a classifier to predict heart failure.\nLoad Samples Loading data from previously saved data can be achieved by\nimport org.apache.spark.mllib.util.MLUtils val data = MLUtils.loadLibSVMFile(sc, \u0026#34;samples\u0026#34;) Basic Statistics Spark MLlib provides various functions to compute summary statistics that are useful when doing machine learning and data analysis tasks.\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics} // colStats() calculates the column statistics for RDD[Vector] // we need to extract only the features part of each LabeledPoint: // RDD[LabeledPoint] =\u0026gt; RDD[Vector] val summary = Statistics.colStats(data.map(_.features)) // summary.mean: a dense vector containing the mean value for each feature (column) // the mean of the first feature is 0.3 summary.mean(0) // the variance of the first feature summary.variance(0) // the number of non-zero values of the first feature summary.numNonzeros(0) Split data In a typical machine learning problem, we need to split data into training (60%) and testing (40%) set.\nval splits = data.randomSplit(Array(0.6, 0.4), seed = 15L) val train = splits(0).cache() val test = splits(1).cache() Train classifier Let\u0026rsquo;s train a linear SVM model using Stochastic Gradient Descent (SGD) on the training set to predict heart failure\nimport org.apache.spark.mllib.classification.SVMWithSGD val numIterations = 100 val model = SVMWithSGD.train(train, numIterations) Testing For each sample in the testing set, output a (prediction, label) pair, and calculate the prediction accuracy. We use the broadcast mechanism to avoid unnecessary data copy.\nval scModel = sc.broadcast(model) val predictionAndLabel = test.map(x =\u0026gt; (scModel.value.predict(x.features), x.label)) val accuracy = predictionAndLabel.filter(x =\u0026gt; x._1 == x._2).count / test.count.toFloat println(\u0026#34;testing Accuracy = \u0026#34; + accuracy) Save \u0026amp; load model In real world setting, you may need to save the trained model. You can achieve that by directly serialize you model object using java ObjectOutputStream and save\nimport java.io.{FileOutputStream, ObjectOutputStream, ObjectInputStream, FileInputStream} // save model  val oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;model\u0026#34;)) oos.writeObject(model) oos.close() // load model from disk  val ois = new ObjectInputStream(new FileInputStream(\u0026#34;model\u0026#34;)) val loadedModel = ois.readObject().asInstanceOf[org.apache.spark.mllib.classification.SVMModel] ois.close() Scikit-learn If typical data set is often small enough after feature construction described in previous Spark Application section, you may consider running machine learning predictive model training and testing using your familiar tools like scikit-learn in Python or some R packages. Here we show how to do that in Scikit-learn, a Python machine learning library.\nFetch data In order to work with Scikit-learn, you will need to take data out of HDFS into a local file system. We can get the samples folder from your home directory in HDFS and merge content into one single file with the command below\nhdfs dfs -getmerge samples patients.svmlight Move on with Python In later steps, you will use python interactive shell. To open a python interactive shell, just type python in bash. You will get prompt similar to the sample below\n[hang@bootcamp1 ~]$ python Python 2.7.10 |Continuum Analytics, Inc.| (default, Oct 19 2015, 18:04:42) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. Anaconda is brought to you by Continuum Analytics. Please check out: http://continuum.io/thanks and https://anaconda.org \u0026gt;\u0026gt;\u0026gt; which show version and distribution of the python installation you are using. Here we pre-installed Anaconda\nLoad and split data Now we can load data and split it into training and testing set in similar way as the MLlib approach.\nfrom sklearn.cross_validation import train_test_split from sklearn.datasets import load_svmlight_file X, y = load_svmlight_file(\u0026#34;patients.svmlight\u0026#34;) X = X.toarray() # make it dense X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=41) Train classifier Let\u0026rsquo;s train a linear SVM model again on the training set to predict heart failure\nfrom sklearn.svm import LinearSVC model = LinearSVC(C=1.0, random_state=42) model.fit(X_train, y_train) Testing We can get prediction accuracy and AUC on testing set as\nfrom sklearn.metrics import roc_auc_score accuracy = model.score(X_test, y_test) y_score = model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print \u0026#34;accuracy = %.3f, AUC = %.3f\u0026#34; % (accuracy, auc) Save \u0026amp; load model We can save and load the trained model via pickle serialization module in Python like\nimport pickle with open(\u0026#39;pysvcmodel.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(model, f) with open(\u0026#39;pysvcmodel.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f: loaded_model = pickle.load(f) Sparsity and predictive features Since we have limited training data but a large number of features, we may consider using L1 penalty on model to regularize parameters.\nfrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) l1_model = LinearSVC(C=1.0, random_state=42, dual=False, penalty=\u0026#39;l1\u0026#39;) l1_model.fit(X_train, y_train) accuracy = l1_model.score(X_test, y_test) y_score = l1_model.decision_function(X_test) auc = roc_auc_score(y_test, y_score) print \u0026#34;for sparse model, accuracy = %.3f, auc = %.3f\u0026#34; % (accuracy, auc) Before fitting a model, we scaled the data to make sure weights of features are comparable. With the sparse model we get from previous example, we can actually identify predictive features according to their coefficients. Here we assume you did the last exercise of previous section about Spark Application. If not, please do that first.\nimport numpy as np ## loading mapping mapping = [] with open(\u0026#39;mapping.txt\u0026#39;) as f: for line in f.readlines(): splits = line.split(\u0026#39;|\u0026#39;) # feature-name | feature-index mapping.append(splits[0]) ## get last 10 - the largest 10 indices top_10 =np.argsort(l1_model.coef_[0])[-10:] for index, fid in enumerate(top_10[::-1]): #read in reverse order print \u0026#34;%d: feature [%s] with coef %.3f\u0026#34; % (index, mapping[fid], l1_model.coef_[0][fid]) Regression Suppose now instead of predicting whether a patient has heart failure, we want to predict the total amount of payment for each patient. This is no longer a binary classification problem, because the labels we try to predict are real-valued numbers. In this case, we can use the regression methods in MLlib.\nConstruct data We need to construct a new dataset for this regression problem. The only difference is that we change the label from heartfailure (binary) to PAYMENT (real value).\nscala\u0026gt; val labelID = featureMap(\u0026#34;PAYMENT\u0026#34;) scala\u0026gt; val data = features.map{ case ((patient, feature), value) =\u0026gt; (patient, (feature, value)) }. groupByKey(). map{ x =\u0026gt; val label = x._2.find(_._1 == labelID).get._2 val featureNoLabel = x._2.toSeq.filter(_._1 != labelID) LabeledPoint(label, Vectors.sparse(numOfFeatures, featureNoLabel)) } Split data Split data into training (60%) and test (40%) set.  scala\u0026gt; val splits = data.randomSplit(Array(0.6, 0.4), seed = 0L) scala\u0026gt; val train = splits(0).cache() scala\u0026gt; val test = splits(1).cache() Training Train a linear regression model using SGD on the training set\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD scala\u0026gt; val numIterations = 100 scala\u0026gt; val model = LinearRegressionWithSGD.train(training, numIterations) Testing For each example in the testing set, output a (prediction, label) pair, and calculate the mean squared error.\nscala\u0026gt; val predictionAndLabel = test.map(x =\u0026gt; (model.predict(x.features), x.label)) scala\u0026gt; val MSE = predictionAndLabel.map{case(p, l) =\u0026gt; math.pow((p - l), 2)}.mean() scala\u0026gt; println(\u0026#34;testing Mean Squared Error = \u0026#34; + MSE) "});index.add({'id':24,'href':'/sections/spark-sql/','title':"Spark Sql",'content':"Spark Sql ::: tip Learning Objectives\n Load data into Spark SQL as DataFrame. Manipulate data with built-in functions. Define a User Defined Function (UDF).  :::\nOverview Recent versions of Spark released the programming abstraction named DataFrame, which can be regarded as a table in a relational database. DataFrame is stored in a distributed manner so that different rows may locate on different machines. On DataFrame you can write sql queries, manipulate columns programatically with API etc.\nLoading data Spark provides an API to load data from JSON, Parquet, Hive table etc. You can refer to the official Spark SQL programming guide for those formats. Here we show how to load csv files. And we will use the spark-csv module by Databricks.\nStart the Spark shell in local mode with the command below to add extra dependencies which are needed to complete this training.\n% spark-shell --master \u0026#34;local[2]\u0026#34; --driver-memory 3G --packages com.databricks:spark-csv_2.11:1.5.0 [logs] Spark context available as sc. 15/05/04 13:12:57 INFO SparkILoop: Created sql context (with Hive support).. SQL context available as sqlContext. scala\u0026gt; ::: tip\nSpark 2.0+ has built-in csv library now. This parameter is not required any more, and it is only used as a sample.\n:::\n::: tip\nYou may want to hide the log messages from spark. You can achieve that by\nimport org.apache.log4j.Logger import org.apache.log4j.Level Logger.getRootLogger.setLevel(Level.ERROR) :::\nNow load data into the shell.\nscala\u0026gt; val sqlContext = spark.sqlContext sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5cef5fc9 scala\u0026gt; val patientEvents = sqlContext.load(\u0026#34;input/\u0026#34;, \u0026#34;com.databricks.spark.csv\u0026#34;). toDF(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;rawvalue\u0026#34;). withColumn(\u0026#34;value\u0026#34;, \u0026#39;rawvalue.cast(\u0026#34;Double\u0026#34;)) patientEvents: org.apache.spark.sql.DataFrame = [patientId: string, eventId: string, date: string, rawvalue: string, value: double] The first parameter is path to the data (in HDFS), and second is a class name, the specific adapter required to load a CSV file. Here we specified a directory name instead of a specific file name so that all files in that directory will be read and combined into one file. Next we call toDF to rename the columns in the CSV file with meaningful names. Finally, we add one more column that has double type of value instead of string which we will use ourselves for the rest of this material.\nManipulating data There are two methods to work with the DataFrame, either using SQL or using domain specific language (DSL).\nSQL Writing SQL is straightforward assuming you have experiences with relational databases.\nscala\u0026gt; patientEvents.registerTempTable(\u0026#34;events\u0026#34;) scala\u0026gt; sqlContext.sql(\u0026#34;select patientId, eventId, count(*) count from events where eventId like \u0026#39;DIAG%\u0026#39; group by patientId, eventId order by count desc\u0026#34;).collect res5: Array[org.apache.spark.sql.Row] = Array(...) Here the patientEvents DataFrame is registered as a table in sql context so that we could run sql commands. Next line is a standard sql command with where, group by and order by statements.\nDSL Next, we show how to manipulate data with DSL, the same result of previous SQL command can be achieved by:\nscala\u0026gt; patientEvents.filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)).groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;).count.orderBy($\u0026#34;count\u0026#34;.desc).show patientId eventId count 00291F39917544B1 DIAG28521 16 00291F39917544B1 DIAG58881 16 00291F39917544B1 DIAG2809 13 00824B6D595BAFB8 DIAG4019 11 0085B4F55FFA358D DIAG28521 9 6A8F2B98C1F6F5DA DIAG58881 8 019E4729585EF3DD DIAG4019 8 0124E58C3460D3F8 DIAG4019 8 2D5D3D5F03C8C176 DIAG4019 8 01A999551906C787 DIAG4019 7 ... For complete DSL functions, see DataFrame class API.\nSaving data Spark SQL provides a convenient way to save data in a different format just like loading data. For example you can write\nscala\u0026gt; patientEvents. filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)). groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;). count. orderBy($\u0026#34;count\u0026#34;.desc). write.json(\u0026#34;aggregated.json\u0026#34;) to save your transformed data in json format or\nscala\u0026gt; patientEvents. filter($\u0026#34;eventId\u0026#34;.startsWith(\u0026#34;DIAG\u0026#34;)). groupBy(\u0026#34;patientId\u0026#34;, \u0026#34;eventId\u0026#34;).count. orderBy($\u0026#34;count\u0026#34;.desc). write.format(\u0026#34;com.databricks.spark.csv\u0026#34;).save(\u0026#34;aggregated.csv\u0026#34;) to save in csv format.\nUDF In many cases the built-in function of SQL like count, max is not enough, you can extend it with your own functions. For example, you want to find the number of different event types with the following UDF.\nDefine Define and register an UDF\nscala\u0026gt; sqlContext.udf.register(\u0026#34;getEventType\u0026#34;, (s: String) =\u0026gt; s match { case diagnostics if diagnostics.startsWith(\u0026#34;DIAG\u0026#34;) =\u0026gt; \u0026#34;diagnostics\u0026#34; case \u0026#34;PAYMENT\u0026#34; =\u0026gt; \u0026#34;payment\u0026#34; case drug if drug.startsWith(\u0026#34;DRUG\u0026#34;) =\u0026gt; \u0026#34;drug\u0026#34; case procedure if procedure.startsWith(\u0026#34;PROC\u0026#34;) =\u0026gt; \u0026#34;procedure\u0026#34; case \u0026#34;heartfailure\u0026#34; =\u0026gt; \u0026#34;heart failure\u0026#34; case _ =\u0026gt; \u0026#34;unknown\u0026#34; }) Use Write sql and call your UDF\nscala\u0026gt; sqlContext.sql(\u0026#34;select getEventType(eventId) type, count(*) count from events group by getEventType(eventId) order by count desc\u0026#34;).show type count drug 16251 diagnostics 10820 payment 3259 procedure 514 heart failure 300  SQL  scala\u0026gt; sqlContext.sql(\u0026#34;select patientId, sum(value) as payment from events where eventId = \u0026#39;PAYMENT\u0026#39; group by patientId order by payment desc limit 10\u0026#34;).show patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0  DSL  scala\u0026gt; patientEvents.filter(patientEvents(\u0026#34;eventId\u0026#34;) === \u0026#34;PAYMENT\u0026#34;).groupBy(\u0026#34;patientId\u0026#34;).agg(\u0026#34;value\u0026#34; -\u0026gt; \u0026#34;sum\u0026#34;).withColumnRenamed(\u0026#34;sum(value)\u0026#34;, \u0026#34;payment\u0026#34;).orderBy($\u0026#34;payment\u0026#34;.desc).show(10) patientId payment 0085B4F55FFA358D 139880.0 019E4729585EF3DD 108980.0 01AC552BE839AB2B 108530.0 0103899F68F866F0 101710.0 00291F39917544B1 99270.0 01A999551906C787 84730.0 01BE015FAF3D32D1 83290.0 002AB71D3224BE66 79850.0 51A115C3BD10C42B 76110.0 01546ADB01630C6C 68190.0 "});index.add({'id':25,'href':'/sections/spark/','title':"Spark",'content':"Overview of Spark ::: tip Learning Objectives\n Learn basics about Scala programming language. Understand Spark RDD operations. Acquire hands-on experiences using Spark for analytics.  :::\nIn this chapter, you will learn about Spark, an in-memory big data computing framework for parallel data processing and analytics. In this training we will illustrate several components of Spark ecosystem through the interactive shell.\nSpark is mainly developed with Scala, a functional programming language on JVM. Though most of the Spark functions also have Python and Java API, we will only present examples in Scala for its conciseness and simplicity. Interested students can learn more about Python and Java in Spark from the official Spark document.\nThis chapter is divided into following sections:\n Scala Basics: You will learn basic Scala syntax via interactive shell, including declaring variables of different types, making function calls as well as how to compile and run a standalone Scala program. Spark Basics: You will learn how to load data into Spark and how to conduct some basic processing, e.g., converting data from raw string into a predefined class, filtering out those items with missing fields and statistics calculation. Spark SQL: You will learn how to use SQL like syntax for data processing in Spark. You will see how the data processing tasks can be achieved with Spark SQL. Spark Application: You will learn how to preprocess data using spark for predictive modeling. Specifically, you will setup patient features and target for later heart failure prediction using MLlib and using Scikit-learn (Python machine learning module). Spark MLlib and Scikit-learn: With the pre-processed data from the previous section, you will have a dataset suitable for Machine Learning tasks. In this section, you will learn how to apply existing algorithms in MLlib and in Scikit-learn to predict whether a patient will have heart failure. Spark GraphX: GraphX is a Spark component for graph data processing. In this section, you will learn how to construct a graph and run graph algorithms such as PageRank, connected components on large graph.  "});index.add({'id':26,'href':'/docs/','title':"Docs",'content':""});})();